{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc90a85",
   "metadata": {},
   "source": [
    "## Loading Config and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed279615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from config.kitti_3d_multi_warmup import *\n",
    "from torchvision import models\n",
    "from Dataset import *\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from py_cpu_nms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df91c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf= Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecdf81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = r'E:\\Thesis_Final\\A2D2_dataset'\n",
    "IMG_DIR = DATASET + \"/images/\"\n",
    "LABEL_DIR = DATASET + \"/A2D2_3D_Obj_det_label_txt/\"\n",
    "Train_CSV_Path= DATASET+ \"/test.csv\"\n",
    "Validation_CSV_Path= DATASET+ \"/subset2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20d288e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 304, 480)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = A2D2_3D_det_Dataset(image_dir= IMG_DIR, label_dir= LABEL_DIR, csv_file= Train_CSV_Path,transform= None)\n",
    "print(np.shape(train_dataset[0].image))\n",
    "val_dataset= A2D2_3D_det_Dataset(image_dir= IMG_DIR, label_dir= LABEL_DIR, csv_file= Validation_CSV_Path,transform= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faebc43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2728\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "train_data= torch.utils.data.DataLoader(train_dataset, batch_size= 1 , shuffle = True, num_workers = 0)\n",
    "val_data= torch.utils.data.DataLoader(val_dataset, batch_size= 1 , shuffle = False, num_workers = 0)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed7d9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_write(file_path, obj):\n",
    "    \"\"\"\n",
    "    Serialize an object to a provided file_path\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "\n",
    "def pickle_read(file_path):\n",
    "    \"\"\"\n",
    "    De-serialize an object from a provided file_path\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b56b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "precomputed = False\n",
    "cache_folder = r\"E:\\Thesis_Final\\Baseline\\M3D-RPN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ca639",
   "metadata": {},
   "source": [
    "## Anchor generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ebe7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbXYWH2Coords(box):\n",
    "    \"\"\"\n",
    "    Convert from [x,y,w,h] to [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "\n",
    "    if box.shape[0] == 0: return np.empty([0,4], dtype=float)\n",
    "\n",
    "    box[:, 2] += box[:, 0] - 1\n",
    "    box[:, 3] += box[:, 1] - 1\n",
    "\n",
    "    return box\n",
    "\n",
    "\n",
    "def determine_ignores(gts, lbls, ilbls, min_gt_vis=0.99, min_gt_h=0, max_gt_h=10e10, scale_factor=1):\n",
    "    \"\"\"\n",
    "    Given various configuration settings, determine which ground truths\n",
    "    are ignored and which are relevant.\n",
    "    \"\"\"\n",
    "\n",
    "    igns = np.zeros([len(gts)], dtype=bool)\n",
    "    rmvs = np.zeros([len(gts)], dtype=bool)\n",
    "\n",
    "    for gtind, gt in enumerate(gts):\n",
    "\n",
    "        ign = gt.ign\n",
    "        ign |= gt.visibility < min_gt_vis\n",
    "        ign |= gt.bbox_full[3] * scale_factor < min_gt_h\n",
    "        ign |= gt.bbox_full[3] * scale_factor > max_gt_h\n",
    "        ign |= gt.cls in ilbls\n",
    "\n",
    "        #rmv = not gt.cls[0] in (lbls + ilbls)\n",
    "        rmv = not gt.cls[0] in (lbls)\n",
    "        igns[gtind] = ign\n",
    "        rmvs[gtind] = rmv\n",
    "\n",
    "    return igns, rmvs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5158a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(box_a, box_b, mode='combinations', data_type=None):\n",
    "    \"\"\"\n",
    "    Computes the amount of intersect between two different sets of boxes.\n",
    "    Args:\n",
    "        box_a (nparray): Mx4 boxes, defined by [x1, y1, x2, y2]\n",
    "        box_a (nparray): Nx4 boxes, defined by [x1, y1, x2, y2]\n",
    "        mode (str): either 'combinations' or 'list', where combinations will check all combinations of box_a and\n",
    "                    box_b hence MxN array, and list expects the same size list M == N, hence returns Mx1 array.\n",
    "        data_type (type): either torch.Tensor or np.ndarray, we automatically determine otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # determine type\n",
    "    if data_type is None: data_type = type(box_a)\n",
    "\n",
    "    # this mode computes the intersect in the sense of combinations.\n",
    "    # i.e., box_a = M x 4, box_b = N x 4 then the output is M x N\n",
    "    if mode == 'combinations':\n",
    "\n",
    "        # np.ndarray\n",
    "        if data_type == np.ndarray:\n",
    "            max_xy = np.minimum(box_a[:, 2:4], np.expand_dims(box_b[:, 2:4], axis=1))\n",
    "            min_xy = np.maximum(box_a[:, 0:2], np.expand_dims(box_b[:, 0:2], axis=1))\n",
    "            inter = np.clip((max_xy - min_xy), a_min=0, a_max=None)\n",
    "\n",
    "        # unknown type\n",
    "        else:\n",
    "            raise ValueError('type {} is not implemented'.format(data_type))\n",
    "\n",
    "        return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "    # this mode computes the intersect in the sense of list_a vs. list_b.\n",
    "    # i.e., box_a = M x 4, box_b = M x 4 then the output is Mx1\n",
    "    elif mode == 'list':\n",
    "\n",
    "        # torch.Tesnor\n",
    "        if data_type == torch.Tensor:\n",
    "            max_xy = torch.min(box_a[:, 2:], box_b[:, 2:])\n",
    "            min_xy = torch.max(box_a[:, :2], box_b[:, :2])\n",
    "            inter = torch.clamp((max_xy - min_xy), 0)\n",
    "\n",
    "        # np.ndarray\n",
    "        elif data_type == np.ndarray:\n",
    "            max_xy = np.min(box_a[:, 2:], box_b[:, 2:])\n",
    "            min_xy = np.max(box_a[:, :2], box_b[:, :2])\n",
    "            inter = np.clip((max_xy - min_xy), a_min=0, a_max=None)\n",
    "\n",
    "        # unknown type\n",
    "        else:\n",
    "            raise ValueError('unknown data type {}'.format(data_type))\n",
    "\n",
    "        return inter[:, 0] * inter[:, 1]\n",
    "\n",
    "    else:\n",
    "        raise ValueError('unknown mode {}'.format(mode))\n",
    "\n",
    "def iou(box_a, box_b, mode='combinations', data_type=None):\n",
    "    \"\"\"\n",
    "    Computes the amount of Intersection over Union (IoU) between two different sets of boxes.\n",
    "    Args:\n",
    "        box_a (nparray): Mx4 boxes, defined by [x1, y1, x2, y2]\n",
    "        box_a (nparray): Nx4 boxes, defined by [x1, y1, x2, y2]\n",
    "        mode (str): either 'combinations' or 'list', where combinations will check all combinations of box_a and\n",
    "                    box_b hence MxN array, and list expects the same size list M == N, hence returns Mx1 array.\n",
    "        data_type (type): either torch.Tensor or np.ndarray, we automatically determine otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # determine type\n",
    "    if data_type is None: data_type = type(box_a)\n",
    "\n",
    "    # this mode computes the IoU in the sense of combinations.\n",
    "    # i.e., box_a = M x 4, box_b = N x 4 then the output is M x N\n",
    "    if mode == 'combinations':\n",
    "\n",
    "        inter = intersect(box_a, box_b, data_type=data_type)\n",
    "        area_a = ((box_a[:, 2] - box_a[:, 0]) *\n",
    "                  (box_a[:, 3] - box_a[:, 1]))\n",
    "        area_b = ((box_b[:, 2] - box_b[:, 0]) *\n",
    "                  (box_b[:, 3] - box_b[:, 1]))\n",
    "        union = np.expand_dims(area_a, 0) + np.expand_dims(area_b, 1) - inter\n",
    "\n",
    "        # torch.Tensor\n",
    "        if data_type == torch.Tensor:\n",
    "            return (inter / union).permute(1, 0)\n",
    "\n",
    "        # np.ndarray\n",
    "        elif data_type == np.ndarray:\n",
    "            return (inter / union).T\n",
    "\n",
    "        # unknown type\n",
    "        else:\n",
    "            raise ValueError('unknown data type {}'.format(data_type))\n",
    "\n",
    "\n",
    "    # this mode compares every box in box_a with target in box_b\n",
    "    # i.e., box_a = M x 4 and box_b = M x 4 then output is M x 1\n",
    "    elif mode == 'list':\n",
    "\n",
    "        inter = intersect(box_a, box_b, mode=mode)\n",
    "        area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])\n",
    "        area_b = (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])\n",
    "        union = area_a + area_b - inter\n",
    "\n",
    "        return inter / union\n",
    "\n",
    "    else:\n",
    "        raise ValueError('unknown mode {}'.format(mode))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2e5c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_mean(x):\n",
    "    sum_all= 0\n",
    "    count_all =0\n",
    "    for i in range(len(x)):\n",
    "        ar= x[i]\n",
    "        sums= 0\n",
    "        count=0\n",
    "        for j in range(len(ar)):\n",
    "            sums+= ar[j]\n",
    "            count+=1\n",
    "        sum_all+=sums\n",
    "        count_all+= count\n",
    "    mean_ar= sum_all/count_all\n",
    "    \n",
    "    return (mean_ar)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f881f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchors(conf, train_data):\n",
    "    \"\"\"\n",
    "    Generates the anchors according to the configuration and\n",
    "    (optionally) based on the imdb properties.\n",
    "    \"\"\"\n",
    "\n",
    "    anchors = np.zeros([len(conf.anchor_scales)*len(conf.anchor_ratios), 4], dtype=np.float32)\n",
    "\n",
    "    aind = 0\n",
    "\n",
    "    # compute simple anchors based on scale/ratios\n",
    "    for scale in conf.anchor_scales:\n",
    "\n",
    "        for ratio in conf.anchor_ratios:\n",
    "\n",
    "            h = scale\n",
    "            w = scale*ratio\n",
    "\n",
    "            anchors[aind, 0:4] = anchor_center(w, h, conf.feat_stride)\n",
    "            aind += 1\n",
    "    #print(\"anchor with 2d initialization\", anchors)\n",
    "    #print(anchors)\n",
    "        # has 3d? then need to compute stats for each new dimension\n",
    "        # presuming that anchors are initialized in \"2d\"\n",
    "    if conf.has_3d:\n",
    "\n",
    "            # compute the default stats for each anchor\n",
    "            normalized_gts = []\n",
    "\n",
    "            # check all images\n",
    "            for imind, (imobj) in enumerate(train_data):\n",
    "                \n",
    "                imobj.H= 1216\n",
    "                imobj.W= 1920\n",
    "                imobj.scale = 1\n",
    "                # has ground truths?\n",
    "                if len(imobj.gts) > 0:\n",
    "\n",
    "                    scale = imobj.scale * conf.test_scale / imobj.H\n",
    "\n",
    "                    # determine ignores\n",
    "                    igns, rmvs = determine_ignores(imobj.gts, conf.lbls, conf.ilbls, conf.min_gt_vis,\n",
    "                                                   conf.min_gt_h, np.inf, scale)\n",
    "\n",
    "                    \n",
    "                    # accumulate boxes\n",
    "                    scaled_gts= np.empty((len(imobj.gts),4))\n",
    "                    j=0 \n",
    "                    while (j <(len(imobj.gts))):\n",
    "                        for gt in imobj.gts:\n",
    "                            for i in range(4):\n",
    "                                scaled_gts[j,i]= gt.bbox_full[i]*scale\n",
    "                            j= j+1\n",
    "                    #    print(np.shape(scaled_gts))\n",
    "                    gts_all =  bbXYWH2Coords(scaled_gts)\n",
    "                    #gts_all = bbXYWH2Coords(np.array([(element * scale for element in gt.bbox_full) for gt in imobj.gts]))\n",
    "                    gts_val = gts_all[(rmvs == False) & (igns == False), :]\n",
    "                    \n",
    "                    #print(\"gts_val\", gts_val)\n",
    "                    gts_3d = np.array([gt.bbox_3d for gt in imobj.gts], dtype= 'object')\n",
    "                    gts_3d = gts_3d[(rmvs == False) & (igns == False), :]\n",
    "                    #print(\"gts_3d\",gts_3d)\n",
    "                    if gts_val.shape[0] > 0:\n",
    "\n",
    "                        # center all 2D ground truths\n",
    "                        for gtind in range(0, gts_val.shape[0]):\n",
    "                            w = gts_val[gtind, 2] - gts_val[gtind, 0] + 1\n",
    "                            h = gts_val[gtind, 3] - gts_val[gtind, 1] + 1\n",
    "\n",
    "                            gts_val[gtind, 0:4] = anchor_center(w, h, conf.feat_stride)\n",
    "                        \n",
    "                    if gts_val.shape[0] > 0:\n",
    "                        normalized_gts += np.concatenate((gts_val, gts_3d), axis=1).tolist()\n",
    "\n",
    "            # convert to np\n",
    "            normalized_gts = np.array(normalized_gts, dtype='object')\n",
    "            #print(\"normalized_gt_shape\",np.shape(normalized_gts))\n",
    "            # expand dimensions\n",
    "            anchors = np.concatenate((anchors, np.zeros([anchors.shape[0], 5])), axis=1)\n",
    "            #print(\"anchors\",anchors)\n",
    "            # bbox_3d order --> [cx3d, cy3d, cz3d, w3d, h3d, l3d, rotY]\n",
    "            anchors_z3d = [[] for x in range(anchors.shape[0])]\n",
    "            anchors_w3d = [[] for x in range(anchors.shape[0])]\n",
    "            anchors_h3d = [[] for x in range(anchors.shape[0])]\n",
    "            anchors_l3d = [[] for x in range(anchors.shape[0])]\n",
    "            anchors_rotY = [[] for x in range(anchors.shape[0])]\n",
    "\n",
    "            # find best matches for each ground truth\n",
    "            ols = iou(anchors[:, 0:4], normalized_gts[:, 0:4])\n",
    "            gt_target_ols = np.amax(ols, axis=0)\n",
    "            gt_target_anchor = np.argmax(ols, axis=0)\n",
    "\n",
    "\n",
    "            # assign each box to an anchor\n",
    "            for gtind, gt in enumerate(normalized_gts):\n",
    "\n",
    "                anum = gt_target_anchor[gtind]\n",
    "\n",
    "                if gt_target_ols[gtind] > 0.2:\n",
    "                    anchors_z3d[anum].append(gt[6])\n",
    "                    anchors_w3d[anum].append(gt[7])\n",
    "                    anchors_h3d[anum].append(gt[8])\n",
    "                    anchors_l3d[anum].append(gt[9])\n",
    "                    anchors_rotY[anum].append(gt[10])\n",
    "\n",
    "\n",
    "            \n",
    "            # compute global means\n",
    "            anchors_z3d_gl = np.empty(0)\n",
    "            anchors_w3d_gl = np.empty(0)\n",
    "            anchors_h3d_gl = np.empty(0)\n",
    "            anchors_l3d_gl = np.empty(0)\n",
    "            anchors_rotY_gl = np.empty(0)\n",
    "\n",
    "            # update anchors\n",
    "            for aind in range(0, anchors.shape[0]):\n",
    "\n",
    "                if len(np.array(anchors_z3d[aind], dtype='object')) > 0:\n",
    "\n",
    "                    if conf.has_3d:\n",
    "\n",
    "                        anchors_z3d_gl = np.hstack((anchors_z3d_gl, np.array(anchors_z3d[aind], dtype= 'object')))\n",
    "                        anchors_w3d_gl = np.hstack((anchors_w3d_gl, np.array(anchors_w3d[aind], dtype= 'object')))\n",
    "                        anchors_h3d_gl = np.hstack((anchors_h3d_gl, np.array(anchors_h3d[aind], dtype= 'object')))\n",
    "                        anchors_l3d_gl = np.hstack((anchors_l3d_gl, np.array(anchors_l3d[aind], dtype= 'object')))\n",
    "                        anchors_rotY_gl = np.hstack((anchors_rotY_gl, np.array(anchors_rotY[aind], dtype= 'object')))\n",
    "\n",
    "\n",
    "                        \n",
    "                        anchors[aind, 4] = array_mean(anchors_z3d_gl)\n",
    "                        anchors[aind, 5] = array_mean(anchors_w3d_gl)\n",
    "                        anchors[aind, 6] = array_mean(anchors_h3d_gl)\n",
    "                        anchors[aind, 7] = array_mean(anchors_l3d_gl)\n",
    "                        anchors[aind, 8] = array_mean(anchors_rotY_gl)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError('Non-used anchor #{} found'.format(aind))\n",
    "    cache_folder = r\"E:\\Thesis_Final\\Baseline\\M3D-RPN\\pickle\"\n",
    "    pickle_write(os.path.join(cache_folder, 'anchors.pkl'), anchors)\n",
    "    conf.anchors = anchors\n",
    "\n",
    "\n",
    "def anchor_center(w, h, stride):\n",
    "    \"\"\"\n",
    "    Centers an anchor based on a stride and the anchor shape (w, h).\n",
    "    center ground truths with steps of half stride\n",
    "    hence box 0 is centered at (7.5, 7.5) rather than (0, 0)\n",
    "    for a feature stride of 16 px.\n",
    "    \"\"\"\n",
    "\n",
    "    anchor = np.zeros([4], dtype=np.float32)\n",
    "\n",
    "    anchor[0] = -w / 2 + (stride - 1) / 2\n",
    "    anchor[1] = -h / 2 + (stride - 1) / 2\n",
    "    anchor[2] = w / 2 + (stride - 1) / 2\n",
    "    anchor[3] = h / 2 + (stride - 1) / 2\n",
    "\n",
    "    return anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487de5a5",
   "metadata": {},
   "source": [
    "## Compute box stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97604311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clsName2Ind(lbls, cls):\n",
    "    \"\"\"\n",
    "    Converts a cls name to an ind\n",
    "    \"\"\"\n",
    "    if cls in lbls:\n",
    "        return lbls.index(cls) + 1\n",
    "    elif not cls:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError('unknown class')\n",
    "        \n",
    "def locate_anchors(anchors, feat_size, stride, convert_tensor=False):\n",
    "    \"\"\"\n",
    "    Spreads each anchor shape across a feature map of size feat_size spaced by a known stride.\n",
    "    Args:\n",
    "        anchors (ndarray): N x 4 array describing [x1, y1, x2, y2] displacements for N anchors\n",
    "        feat_size (ndarray): the downsampled resolution W x H to spread anchors across\n",
    "        stride (int): stride of a network\n",
    "        convert_tensor (bool, optional): whether to return a torch tensor, otherwise ndarray [default=False]\n",
    "    Returns:\n",
    "         ndarray: 2D array = [(W x H) x 5] array consisting of [x1, y1, x2, y2, anchor_index]\n",
    "    \"\"\"\n",
    "\n",
    "    # compute rois\n",
    "    shift_x = np.array(range(0, feat_size[1], 1)) * float(stride)\n",
    "    shift_y = np.array(range(0, feat_size[0], 1)) * float(stride)\n",
    "    [shift_x, shift_y] = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "    rois = np.expand_dims(anchors[:, 0:4], axis=1)\n",
    "    shift_x = np.expand_dims(shift_x, axis=0)\n",
    "    shift_y = np.expand_dims(shift_y, axis=0)\n",
    "\n",
    "    shift_x1 = shift_x + np.expand_dims(rois[:, :, 0], axis=2)\n",
    "    shift_y1 = shift_y + np.expand_dims(rois[:, :, 1], axis=2)\n",
    "    shift_x2 = shift_x + np.expand_dims(rois[:, :, 2], axis=2)\n",
    "    shift_y2 = shift_y + np.expand_dims(rois[:, :, 3], axis=2)\n",
    "\n",
    "    # compute anchor tracker\n",
    "    anchor_tracker = np.zeros(shift_x1.shape, dtype=float)\n",
    "    for aind in range(0, rois.shape[0]): anchor_tracker[aind, :, :] = aind\n",
    "\n",
    "    stack_size = feat_size[0] * anchors.shape[0]\n",
    "\n",
    "    # torch and numpy MAY have different calls for reshaping, although\n",
    "    # it is not very important which is used as long as it is CONSISTENT\n",
    "    if convert_tensor:\n",
    "\n",
    "        # important to unroll according to pytorch\n",
    "        shift_x1 = torch.from_numpy(shift_x1).view(1, stack_size, feat_size[1])\n",
    "        shift_y1 = torch.from_numpy(shift_y1).view(1, stack_size, feat_size[1])\n",
    "        shift_x2 = torch.from_numpy(shift_x2).view(1, stack_size, feat_size[1])\n",
    "        shift_y2 = torch.from_numpy(shift_y2).view(1, stack_size, feat_size[1])\n",
    "        anchor_tracker = torch.from_numpy(anchor_tracker).view(1, stack_size, feat_size[1])\n",
    "\n",
    "        shift_x1.requires_grad = False\n",
    "        shift_y1.requires_grad = False\n",
    "        shift_x2.requires_grad = False\n",
    "        shift_y2.requires_grad = False\n",
    "        anchor_tracker.requires_grad = False\n",
    "\n",
    "        shift_x1 = shift_x1.permute(1, 2, 0).contiguous().view(-1, 1)\n",
    "        shift_y1 = shift_y1.permute(1, 2, 0).contiguous().view(-1, 1)\n",
    "        shift_x2 = shift_x2.permute(1, 2, 0).contiguous().view(-1, 1)\n",
    "        shift_y2 = shift_y2.permute(1, 2, 0).contiguous().view(-1, 1)\n",
    "        anchor_tracker = anchor_tracker.permute(1, 2, 0).contiguous().view(-1, 1)\n",
    "\n",
    "        rois = torch.cat((shift_x1, shift_y1, shift_x2, shift_y2, anchor_tracker), 1)\n",
    "\n",
    "    else:\n",
    "\n",
    "        shift_x1 = shift_x1.reshape(1, stack_size, feat_size[1]).reshape(-1, 1)\n",
    "        shift_y1 = shift_y1.reshape(1, stack_size, feat_size[1]).reshape(-1, 1)\n",
    "        shift_x2 = shift_x2.reshape(1, stack_size, feat_size[1]).reshape(-1, 1)\n",
    "        shift_y2 = shift_y2.reshape(1, stack_size, feat_size[1]).reshape(-1, 1)\n",
    "        anchor_tracker = anchor_tracker.reshape(1, stack_size, feat_size[1]).reshape(-1, 1)\n",
    "\n",
    "        rois = np.concatenate((shift_x1, shift_y1, shift_x2, shift_y2, anchor_tracker), 1)\n",
    "\n",
    "    return rois\n",
    "\n",
    "def bbox_transform(ex_rois, gt_rois):\n",
    "    \"\"\"\n",
    "    Compute the bbox target transforms in 2D.\n",
    "    Translations are done as simple difference, whereas others involving\n",
    "    scaling are done in log space (hence, log(1) = 0, log(0.8) < 0 and\n",
    "    log(1.2) > 0 which is a good property).\n",
    "    \"\"\"\n",
    "\n",
    "    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n",
    "    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n",
    "    ex_ctr_x = ex_rois[:, 0] + 0.5 * (ex_widths - 1)\n",
    "    ex_ctr_y = ex_rois[:, 1] + 0.5 * (ex_heights - 1)\n",
    "\n",
    "    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n",
    "    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n",
    "    gt_ctr_x = gt_rois[:, 0] + 0.5 * (gt_widths - 1.0)\n",
    "    gt_ctr_y = gt_rois[:, 1] + 0.5 * (gt_heights - 1.0)\n",
    "\n",
    "    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n",
    "    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n",
    "    targets_dw = np.log(gt_widths / ex_widths)\n",
    "    targets_dh = np.log(gt_heights / ex_heights)\n",
    "\n",
    "    targets = np.vstack((targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n",
    "\n",
    "    return targets\n",
    "\n",
    "\n",
    "def bbox_transform_3d(ex_rois_2d, ex_rois_3d, gt_rois):\n",
    "    \"\"\"\n",
    "    Compute the bbox target transforms in 3D.\n",
    "    Translations are done as simple difference, whereas others involving\n",
    "    scaling are done in log space (hence, log(1) = 0, log(0.8) < 0 and\n",
    "    log(1.2) > 0 which is a good property).\n",
    "    \"\"\"\n",
    "\n",
    "    ex_widths = ex_rois_2d[:, 2] - ex_rois_2d[:, 0] + 1.0\n",
    "    ex_heights = ex_rois_2d[:, 3] - ex_rois_2d[:, 1] + 1.0\n",
    "    ex_ctr_x = ex_rois_2d[:, 0] + 0.5 * (ex_widths - 1)\n",
    "    ex_ctr_y = ex_rois_2d[:, 1] + 0.5 * (ex_heights - 1)\n",
    "\n",
    "    gt_ctr_x = gt_rois[:, 0]\n",
    "    gt_ctr_y = gt_rois[:, 1]\n",
    "\n",
    "    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n",
    "    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n",
    "\n",
    "    delta_z = gt_rois[:, 2] - ex_rois_3d[:, 0]\n",
    "    scale_w = np.log(gt_rois[:, 3] / ex_rois_3d[:, 1])\n",
    "    scale_h = np.log(gt_rois[:, 4] / ex_rois_3d[:, 2])\n",
    "    scale_l = np.log(gt_rois[:, 5] / ex_rois_3d[:, 3])\n",
    "    deltaRotY = gt_rois[:, 6] - ex_rois_3d[:, 4]\n",
    "\n",
    "    targets = np.vstack((targets_dx, targets_dy, delta_z, scale_w, scale_h, scale_l, deltaRotY)).transpose()\n",
    "    targets = np.hstack((targets, gt_rois[:, 7:]))\n",
    "\n",
    "\n",
    "    return targets\n",
    "\n",
    "def iou_ign(box_a, box_b, mode='combinations', data_type=None):\n",
    "    \"\"\"\n",
    "    Computes the amount of overap of box_b has within box_a, which is handy for dealing with ignore regions.\n",
    "    Hence, assume that box_b are ignore regions and box_a are anchor boxes, then we may want to know how\n",
    "    much overlap the anchors have inside of the ignore regions (hence ignore area_b!)\n",
    "    Args:\n",
    "        box_a (nparray): Mx4 boxes, defined by [x1, y1, x2, y2]\n",
    "        box_a (nparray): Nx4 boxes, defined by [x1, y1, x2, y2]\n",
    "        mode (str): either 'combinations' or 'list', where combinations will check all combinations of box_a and\n",
    "                    box_b hence MxN array, and list expects the same size list M == N, hence returns Mx1 array.\n",
    "        data_type (type): either torch.Tensor or np.ndarray, we automatically determine otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    if data_type is None: data_type = type(box_a)\n",
    "\n",
    "    # this mode computes the IoU in the sense of combinations.\n",
    "    # i.e., box_a = M x 4, box_b = N x 4 then the output is M x N\n",
    "    if mode == 'combinations':\n",
    "\n",
    "        inter = intersect(box_a, box_b, data_type=data_type)\n",
    "        area_a = ((box_a[:, 2] - box_a[:, 0]) *\n",
    "                  (box_a[:, 3] - box_a[:, 1]))\n",
    "        area_b = ((box_b[:, 2] - box_b[:, 0]) *\n",
    "                  (box_b[:, 3] - box_b[:, 1]))\n",
    "        union = np.expand_dims(area_a, 0) + np.expand_dims(area_b, 1) * 0 - inter * 0\n",
    "\n",
    "        # torch and numpy have different calls for transpose\n",
    "        if data_type == torch.Tensor:\n",
    "            return (inter / union).permute(1, 0)\n",
    "        elif data_type == np.ndarray:\n",
    "            return (inter / union).T\n",
    "\n",
    "        # unknown type\n",
    "        else:\n",
    "            raise ValueError('unknown data type {}'.format(data_type))\n",
    "\n",
    "    else:\n",
    "        raise ValueError('unknown mode {}'.format(mode))\n",
    "        \n",
    "def compute_targets(gts_val, gts_ign, box_lbls, rois, fg_thresh, ign_thresh, bg_thresh_lo, bg_thresh_hi, best_thresh,\n",
    "                    gts_3d=None, anchors=[], tracker=[]):\n",
    "    \"\"\"\n",
    "    Computes the bbox targets of a set of rois and a set\n",
    "    of ground truth boxes, provided various ignore\n",
    "    settings in configuration\n",
    "    \"\"\"\n",
    "\n",
    "    ols = None\n",
    "    has_3d = gts_3d is not None\n",
    "\n",
    "    # init transforms which respectively hold [dx, dy, dw, dh, label]\n",
    "    # for labels bg=-1, ign=0, fg>=1\n",
    "    transforms = np.zeros([len(rois), 5], dtype=np.float32)\n",
    "    raw_gt = np.zeros([len(rois), 5], dtype=np.float32)\n",
    "\n",
    "    # if 3d, then init other terms after\n",
    "    if has_3d:\n",
    "        transforms = np.pad(transforms, [(0, 0), (0, gts_3d.shape[1])], 'constant')\n",
    "        raw_gt = np.pad(raw_gt, [(0, 0), (0, gts_3d.shape[1])], 'constant')\n",
    "\n",
    "    if gts_val.shape[0] > 0 or gts_ign.shape[0] > 0:\n",
    "        #rint(\"gts_val_shape loop triggered\")\n",
    "\n",
    "        if gts_ign.shape[0] > 0:\n",
    "\n",
    "            # compute overlaps ign\n",
    "            #print(\"roi\",rois.shape)\n",
    "            #print(\"gts\", gts_ign.shape)\n",
    "            ols_ign = iou_ign(rois, gts_ign)\n",
    "            #print(\"ols_ign\",ols_ign.shape)\n",
    "            ols_ign_max = np.amax(ols_ign, axis=1)\n",
    "\n",
    "        else:\n",
    "            ols_ign_max = np.zeros([rois.shape[0]], dtype=np.float32)\n",
    "\n",
    "        if gts_val.shape[0] > 0:\n",
    "\n",
    "            # compute overlaps valid\n",
    "            ols = iou(rois, gts_val)\n",
    "            ols_max = np.amax(ols, axis=1)\n",
    "            targets = np.argmax(ols, axis=1)\n",
    "\n",
    "            # find best matches for each ground truth\n",
    "            gt_best_rois = np.argmax(ols, axis=0)\n",
    "            gt_best_ols = np.amax(ols, axis=0)\n",
    "\n",
    "            gt_best_rois = gt_best_rois[gt_best_ols >= best_thresh]\n",
    "            gt_best_ols = gt_best_ols[gt_best_ols >= best_thresh]\n",
    "\n",
    "            fg_inds = np.flatnonzero(ols_max >= fg_thresh)\n",
    "            fg_inds = np.concatenate((fg_inds, gt_best_rois))\n",
    "            fg_inds = np.unique(fg_inds)\n",
    "            #print(fg_inds)\n",
    "\n",
    "            target_rois = gts_val[targets[fg_inds], :]\n",
    "            src_rois = rois[fg_inds, :]\n",
    "            #print(\"target_rois\",target_rois)\n",
    "            #print(\"target_rois\",src_rois)\n",
    "            if len(fg_inds) > 0:\n",
    "                \n",
    "                # compute 2d transform\n",
    "                transforms[fg_inds, 0:4] = bbox_transform(src_rois, target_rois)\n",
    "                #print(\"transforms[fg_inds, 0:4]\",transforms[fg_inds, 0:4])\n",
    "                raw_gt[fg_inds, 0:4] = target_rois\n",
    "\n",
    "                if has_3d:\n",
    "\n",
    "                    tracker = tracker.astype(np.int64)\n",
    "                    #print(\"tracker\",tracker)\n",
    "                    src_3d = anchors[tracker[fg_inds], 4:]\n",
    "                    target_3d = gts_3d[targets[fg_inds]]\n",
    "\n",
    "                    raw_gt[fg_inds, 5:] = target_3d\n",
    "                    \n",
    "                    \n",
    "                    # compute 3d transform\n",
    "                    transforms[fg_inds, 5:] = bbox_transform_3d(src_rois, src_3d, target_3d)\n",
    "                    #print(\"transforms[fg_inds, 5:]\",transforms[fg_inds, 5:])\n",
    "\n",
    "                # store labels\n",
    "                transforms[fg_inds, 4] = [box_lbls[x] for x in targets[fg_inds]]\n",
    "                assert (all(transforms[fg_inds, 4] >= 1))\n",
    "                #print(\"transforms[:,4]\",transforms[:,4])\n",
    "          \n",
    "        else:\n",
    "\n",
    "            ols_max = np.zeros(rois.shape[0], dtype=int)\n",
    "            fg_inds = np.empty(shape=[0])\n",
    "            gt_best_rois = np.empty(shape=[0])\n",
    "\n",
    "        # determine ignores\n",
    "        ign_inds = np.flatnonzero(ols_ign_max >= ign_thresh)\n",
    "\n",
    "        # determine background\n",
    "        bg_inds = np.flatnonzero((ols_max >= bg_thresh_lo) & (ols_max < bg_thresh_hi))\n",
    "\n",
    "        # subtract fg and igns from background\n",
    "        bg_inds = np.setdiff1d(bg_inds, ign_inds)\n",
    "        bg_inds = np.setdiff1d(bg_inds, fg_inds)\n",
    "        bg_inds = np.setdiff1d(bg_inds, gt_best_rois)\n",
    "\n",
    "        # mark background\n",
    "        transforms[bg_inds, 4] = -1\n",
    "    else:\n",
    "\n",
    "        # all background\n",
    "        transforms[:, 4] = -1\n",
    "\n",
    "    \n",
    "    return transforms, ols, raw_gt\n",
    "\n",
    "\n",
    "def calc_output_size(res, stride):\n",
    "    \"\"\"\n",
    "    Approximate the output size of a network\n",
    "    Args:\n",
    "        res (ndarray): input resolution\n",
    "        stride (int): stride of a network\n",
    "    Returns:\n",
    "         ndarray: output resolution\n",
    "    \"\"\"\n",
    "\n",
    "    return np.ceil(np.array(res)/stride).astype(int)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f41de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "627fc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bbox_stats(conf, imdb):\n",
    "    \"\"\"\n",
    "    Computes the mean and standard deviation for each regression\n",
    "    parameter (usually pertaining to [dx, dy, sw, sh] but sometimes\n",
    "    for 3d parameters too).\n",
    "    Once these stats are known we normalize the regression targets\n",
    "    to have 0 mean and 1 variance, to hypothetically ease training.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if conf.has_3d:\n",
    "        squared_sums = np.zeros([1, 11], dtype=np.float64)\n",
    "        sums = np.zeros([1, 11], dtype=np.float64)\n",
    "    else:\n",
    "        squared_sums = np.zeros([1, 4], dtype=np.float64)\n",
    "        sums = np.zeros([1, 4], dtype=np.float64)\n",
    "\n",
    "    class_counts = np.zeros([1], dtype=np.float64) + 1e-10\n",
    "\n",
    "    # compute the mean first\n",
    "    #logging.info('Computing bbox regression mean..')\n",
    "\n",
    "    for imind, imobj in enumerate(train_data):\n",
    "        imobj.imH= 1216\n",
    "        imobj.imW= 1920\n",
    "        imobj.scale = 1\n",
    "        # has ground truths?\n",
    "        if len(imobj.gts) > 0:\n",
    "\n",
    "            scale_factor = imobj.scale * conf.test_scale / imobj.imH\n",
    "            #print(scale_factor)\n",
    "            feat_size = calc_output_size(np.array([imobj.imH, imobj.imW]) * scale_factor, conf.feat_stride)\n",
    "            rois = locate_anchors(conf.anchors, feat_size, conf.feat_stride)\n",
    "            #print(rois.shape)\n",
    "            #print(\"rois\",rois)\n",
    "            # determine ignores\n",
    "            igns, rmvs = determine_ignores(imobj.gts, conf.lbls, conf.ilbls, conf.min_gt_vis,\n",
    "                                            conf.min_gt_h, np.inf, scale_factor)\n",
    "\n",
    "            # accumulate boxes\n",
    "            scaled_gts= np.empty((len(imobj.gts),4))\n",
    "            j=0 \n",
    "            while (j <(len(imobj.gts))):\n",
    "                for gt in imobj.gts:\n",
    "                    for i in range(4):\n",
    "                        scaled_gts[j,i]= gt.bbox_full[i]*scale_factor\n",
    "                    j= j+1\n",
    "            #print(np.shape(scaled_gts))\n",
    "            gts_all =  bbXYWH2Coords(scaled_gts)\n",
    "            #print(\"gts_allshape\", gts_all.shape)\n",
    "            \n",
    "            # filter out irrelevant cls, and ignore cls\n",
    "            gts_val = gts_all[(rmvs == False) & (igns == False), :]\n",
    "            gts_ign = gts_all[(rmvs == False) & (igns == True), :]\n",
    "            \n",
    "            #print(\"gts_val_shape\", gts_val.shape)\n",
    "            #print(\"gts_ign_shape\", gts_ign.shape)\n",
    "            # accumulate labels\n",
    "            box_lbls = np.array([gt.cls[0] for gt in imobj.gts])\n",
    "            box_lbls = box_lbls[(rmvs == False) & (igns == False)]\n",
    "            \n",
    "            box_lbls = np.array([clsName2Ind(conf.lbls, cls) for cls in box_lbls])\n",
    "            #print(box_lbls)\n",
    "            #print(\"box_lbls_shape\", box_lbls.shape)\n",
    "            \n",
    "            #print(\"gts_val\",gts_val)\n",
    "            #print(\"box_lbls\",box_lbls)\n",
    "            if conf.has_3d:\n",
    "\n",
    "                # accumulate 3d boxes\n",
    "                gts_3d = np.array([gt.bbox_3d for gt in imobj.gts], dtype='object')\n",
    "                gts_3d = gts_3d[(rmvs == False) & (igns == False), :]\n",
    "\n",
    "                # rescale centers (in 2d)\n",
    "                for gtind, gt in enumerate(gts_3d):\n",
    "                    gts_3d[gtind, 0:2] *= scale_factor\n",
    "                #print(\"gts_3d\",gts_3d.shape)\n",
    "                # compute transforms for all 3d\n",
    "                transforms, _, _= compute_targets(gts_val, gts_ign, box_lbls, rois, conf.fg_thresh, conf.ign_thresh,\n",
    "                                                conf.bg_thresh_lo, conf.bg_thresh_hi, conf.best_thresh, gts_3d=gts_3d,\n",
    "                                                anchors=conf.anchors, tracker=rois[:, 4])\n",
    "            else:\n",
    "\n",
    "                # compute transforms for 2d\n",
    "                transforms, _, _ = compute_targets(gts_val, gts_ign, box_lbls, rois, conf.fg_thresh, conf.ign_thresh,\n",
    "                                                conf.bg_thresh_lo, conf.bg_thresh_hi, conf.best_thresh)\n",
    "\n",
    "            \n",
    "            #= (np.isnan(transforms))\n",
    "            #for i in range(transforms.shape[0]):\n",
    "            #    for j in range(transforms.shape[1]):\n",
    "            #        if (math.isnan(transforms[i,j])):\n",
    "            #            print(\"transforms\",(i,j))\n",
    "            #        else:\n",
    "            #            pass\n",
    "            gt_inds = np.flatnonzero(transforms[:, 4] > 0)\n",
    "            \n",
    "            if len(gt_inds) > 0:\n",
    "\n",
    "                if conf.has_3d:\n",
    "\n",
    "                    sums[:, 0:4] += np.sum(transforms[gt_inds, 0:4], axis=0)\n",
    "                    sums[:, 4:] += np.sum(transforms[gt_inds, 5:12], axis=0)\n",
    "                else:\n",
    "                    sums += np.sum(transforms[gt_inds, 0:4], axis=0)\n",
    "            \n",
    "                class_counts += len(gt_inds)\n",
    "                #print(\"sums\",sums)\n",
    "    means = sums/class_counts\n",
    "\n",
    "    #logging.info('Computing bbox regression stds..')\n",
    "\n",
    "    for imind, imobj in enumerate(train_data):\n",
    "        imobj.imH= 1216\n",
    "        imobj.imW= 1920\n",
    "        imobj.scale = 1\n",
    "        # has ground truths?\n",
    "        if len(imobj.gts) > 0:\n",
    "            scale_factor = imobj.scale * conf.test_scale / imobj.imH\n",
    "            feat_size = calc_output_size(np.array([imobj.imH, imobj.imW]) * scale_factor, conf.feat_stride)\n",
    "            rois = locate_anchors(conf.anchors, feat_size, conf.feat_stride)\n",
    "\n",
    "            # determine ignores\n",
    "            igns, rmvs = determine_ignores(imobj.gts, conf.lbls, conf.ilbls, conf.min_gt_vis,\n",
    "                                            conf.min_gt_h, np.inf, scale_factor)\n",
    "\n",
    "            # accumulate boxes\n",
    "            scaled_gts= np.empty((len(imobj.gts),4))\n",
    "            j=0 \n",
    "            while (j <(len(imobj.gts))):\n",
    "                for gt in imobj.gts:\n",
    "                    for i in range(4):\n",
    "                        scaled_gts[j,i]= gt.bbox_full[i]*scale_factor\n",
    "                    j= j+1\n",
    "            #print(np.shape(scaled_gts))\n",
    "            gts_all =  bbXYWH2Coords(scaled_gts)\n",
    "            \n",
    "            # filter out irrelevant cls, and ignore cls\n",
    "            gts_val = gts_all[(rmvs == False) & (igns == False), :]\n",
    "            gts_ign = gts_all[(rmvs == False) & (igns == True), :]\n",
    "            \n",
    "            \n",
    "            \n",
    "            # accumulate labels\n",
    "            box_lbls = np.array([gt.cls[0] for gt in imobj.gts])\n",
    "            box_lbls = box_lbls[(rmvs == False) & (igns == False)]\n",
    "            box_lbls = np.array([clsName2Ind(conf.lbls, cls) for cls in box_lbls])\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            if conf.has_3d:\n",
    "\n",
    "                # accumulate 3d boxes\n",
    "                gts_3d = np.array([gt.bbox_3d for gt in imobj.gts], dtype='object')\n",
    "                gts_3d = gts_3d[(rmvs == False) & (igns == False), :]\n",
    "\n",
    "                # rescale centers (in 2d)\n",
    "                for gtind, gt in enumerate(gts_3d):\n",
    "                    gts_3d[gtind, 0:2] *= scale_factor\n",
    "\n",
    "                # compute transforms for all 3d\n",
    "                transforms, _, _ = compute_targets(gts_val, gts_ign, box_lbls, rois, conf.fg_thresh, conf.ign_thresh,\n",
    "                                                conf.bg_thresh_lo, conf.bg_thresh_hi, conf.best_thresh, gts_3d=gts_3d,\n",
    "                                                anchors=conf.anchors, tracker=rois[:, 4])\n",
    "                \n",
    "            else:\n",
    "\n",
    "                # compute transforms for 2d\n",
    "                transforms, _, _ = compute_targets(gts_val, gts_ign, box_lbls, rois, conf.fg_thresh, conf.ign_thresh,\n",
    "                                                conf.bg_thresh_lo, conf.bg_thresh_hi, conf.best_thresh)\n",
    "                \n",
    "            \n",
    "            gt_inds = np.flatnonzero(transforms[:, 4] > 0)\n",
    "\n",
    "            if len(gt_inds) > 0:\n",
    "\n",
    "                if conf.has_3d:\n",
    "\n",
    "                    squared_sums[:, 0:4] += np.sum(np.power(transforms[gt_inds, 0:4] - means[:, 0:4], 2), axis=0)\n",
    "                    squared_sums[:, 4:] += np.sum(np.power(transforms[gt_inds, 5:12] - means[:, 4:], 2), axis=0)\n",
    "\n",
    "                else:\n",
    "                    squared_sums += np.sum(np.power(transforms[gt_inds, 0:4] - means, 2), axis=0)\n",
    "\n",
    "\n",
    "        stds = np.sqrt((squared_sums/class_counts))\n",
    "\n",
    "        means = means.astype(float)\n",
    "        stds = stds.astype(float)\n",
    "    cache_folder = r\"E:\\Thesis_Final\\Baseline\\M3D-RPN\\pickle\"\n",
    "    pickle_write(os.path.join(cache_folder, 'bbox_means.pkl'), means)\n",
    "    pickle_write(os.path.join(cache_folder, 'bbox_stds.pkl'), stds)\n",
    "    conf.bbox_means = means\n",
    "    conf.bbox_stds = stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49696033",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I009809\\AppData\\Local\\Temp\\ipykernel_17664\\1958609662.py:60: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  gts_3d = np.array([gt.bbox_3d for gt in imobj.gts], dtype= 'object')\n",
      "C:\\Users\\I009809\\AppData\\Local\\Temp\\ipykernel_17664\\1958609662.py:118: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  if len(np.array(anchors_z3d[aind], dtype='object')) > 0:\n",
      "C:\\Users\\I009809\\AppData\\Local\\Temp\\ipykernel_17664\\1958609662.py:122: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  anchors_z3d_gl = np.hstack((anchors_z3d_gl, np.array(anchors_z3d[aind], dtype= 'object')))\n",
      "C:\\Users\\I009809\\AppData\\Local\\Temp\\ipykernel_17664\\1958609662.py:123: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  anchors_w3d_gl = np.hstack((anchors_w3d_gl, np.array(anchors_w3d[aind], dtype= 'object')))\n",
      "C:\\Users\\I009809\\AppData\\Local\\Temp\\ipykernel_17664\\1958609662.py:124: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  anchors_h3d_gl = np.hstack((anchors_h3d_gl, np.array(anchors_h3d[aind], dtype= 'object')))\n",
      "C:\\Users\\I009809\\AppData\\Local\\Temp\\ipykernel_17664\\1958609662.py:125: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  anchors_l3d_gl = np.hstack((anchors_l3d_gl, np.array(anchors_l3d[aind], dtype= 'object')))\n",
      "C:\\Users\\I009809\\AppData\\Local\\Temp\\ipykernel_17664\\1958609662.py:126: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  anchors_rotY_gl = np.hstack((anchors_rotY_gl, np.array(anchors_rotY[aind], dtype= 'object')))\n",
      "C:\\Users\\I009809\\AppData\\Local\\Temp\\ipykernel_17664\\3371130520.py:71: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  gts_3d = np.array([gt.bbox_3d for gt in imobj.gts], dtype='object')\n",
      "C:\\Users\\I009809\\AppData\\Local\\Temp\\ipykernel_17664\\3371130520.py:155: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  gts_3d = np.array([gt.bbox_3d for gt in imobj.gts], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "if precomputed == True:\n",
    "    conf.anchors = pickle_read(os.path.join(cache_folder, 'anchors.pkl'))\n",
    "    conf.bbox_means = pickle_read(os.path.join(cache_folder, 'bbox_means.pkl'))\n",
    "    conf.bbox_stds = pickle_read(os.path.join(cache_folder, 'bbox_stds.pkl'))\n",
    "else:\n",
    "    generate_anchors(conf, train_data)\n",
    "    compute_bbox_stats(conf, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6593a750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 9)\n",
      "[[ 2.75000000e+00 -2.00000000e+00  1.22500000e+01  1.70000000e+01\n",
      "   3.49857119e+01  1.79552632e+00  6.10789474e-01  6.41052632e-01\n",
      "   1.66637910e+00]\n",
      " [-2.00000000e+00 -2.00000000e+00  1.70000000e+01  1.70000000e+01\n",
      "   3.88318435e+01  1.86930556e+00  1.15875000e+00  1.89500000e+00\n",
      "   1.25005681e+00]\n",
      " [-6.75000000e+00 -2.00000000e+00  2.17500000e+01  1.70000000e+01\n",
      "   3.81103272e+01  1.74174699e+00  1.65861446e+00  2.99831325e+00\n",
      "   1.14491234e+00]\n",
      " [ 1.54610741e+00 -4.40778542e+00  1.34538927e+01  1.94077854e+01\n",
      "   3.47335027e+01  1.78493333e+00  1.21913333e+00  1.98373333e+00\n",
      "   1.38964248e+00]\n",
      " [-4.40778542e+00 -4.40778542e+00  1.94077854e+01  1.94077854e+01\n",
      "   3.58599046e+01  1.84437126e+00  1.48544910e+00  2.63844311e+00\n",
      "   1.09346065e+00]\n",
      " [-1.03616781e+01 -4.40778542e+00  2.53616772e+01  1.94077854e+01\n",
      "   3.45986801e+01  1.78416667e+00  1.69203980e+00  3.11895522e+00\n",
      "   1.14408502e+00]\n",
      " [ 3.70869525e-02 -7.42582607e+00  1.49629135e+01  2.24258270e+01\n",
      "   3.30736467e+01  1.79848610e+00  1.54660144e+00  2.80359423e+00\n",
      "   1.17187817e+00]\n",
      " [-7.42582607e+00 -7.42582607e+00  2.24258270e+01  2.24258270e+01\n",
      "   3.26724059e+01  1.84126382e+00  1.63239336e+00  3.08616904e+00\n",
      "   1.05096402e+00]\n",
      " [-1.48887396e+01 -7.42582607e+00  2.98887386e+01  2.24258270e+01\n",
      "   3.11537353e+01  1.79588816e+00  1.75833882e+00  3.36233004e+00\n",
      "   1.13414969e+00]\n",
      " [-1.85439634e+00 -1.12087927e+01  1.68543968e+01  2.62087936e+01\n",
      "   3.02427736e+01  1.80150473e+00  1.67325361e+00  3.16475336e+00\n",
      "   1.13618891e+00]\n",
      " [-1.12087927e+01 -1.12087927e+01  2.62087936e+01  2.62087936e+01\n",
      "   3.00563116e+01  1.86390936e+00  1.74417620e+00  3.39767471e+00\n",
      "   1.04387787e+00]\n",
      " [-2.05631886e+01 -1.12087927e+01  3.55631905e+01  2.62087936e+01\n",
      "   2.84660398e+01  1.83212924e+00  1.82357997e+00  3.58886591e+00\n",
      "   1.09015708e+00]\n",
      " [-4.22527838e+00 -1.59505568e+01  1.92252789e+01  3.09505558e+01\n",
      "   2.79815323e+01  1.83593654e+00  1.77753851e+00  3.48772643e+00\n",
      "   1.09017206e+00]\n",
      " [-1.59505568e+01 -1.59505568e+01  3.09505558e+01  3.09505558e+01\n",
      "   2.75096137e+01  1.87250834e+00  1.81447998e+00  3.60363181e+00\n",
      "   1.02460484e+00]\n",
      " [-2.76758347e+01 -1.59505568e+01  4.26758347e+01  3.09505558e+01\n",
      "   2.62572975e+01  1.85156817e+00  1.85424383e+00  3.73788506e+00\n",
      "   1.06782930e+00]\n",
      " [-7.19706249e+00 -2.18941250e+01  2.21970615e+01  3.68941231e+01\n",
      "   2.59149676e+01  1.85231569e+00  1.82548168e+00  3.66768204e+00\n",
      "   1.06848118e+00]\n",
      " [-2.18941250e+01 -2.18941250e+01  3.68941231e+01  3.68941231e+01\n",
      "   2.55170548e+01  1.87997880e+00  1.84818105e+00  3.73417850e+00\n",
      "   1.04263977e+00]\n",
      " [-3.65911865e+01 -2.18941250e+01  5.15911865e+01  3.68941231e+01\n",
      "   2.44691878e+01  1.86396522e+00  1.86822151e+00  3.81020979e+00\n",
      "   1.05947858e+00]\n",
      " [-1.09220486e+01 -2.93440971e+01  2.59220486e+01  4.43440971e+01\n",
      "   2.43208148e+01  1.86746739e+00  1.85686545e+00  3.79196049e+00\n",
      "   1.05628070e+00]\n",
      " [-2.93440971e+01 -2.93440971e+01  4.43440971e+01  4.43440971e+01\n",
      "   2.39790418e+01  1.88151586e+00  1.86549543e+00  3.82178821e+00\n",
      "   1.04499890e+00]\n",
      " [-4.77661476e+01 -2.93440971e+01  6.27661476e+01  4.43440971e+01\n",
      "   2.32701793e+01  1.87211213e+00  1.87877490e+00  3.87495900e+00\n",
      "   1.04564864e+00]\n",
      " [-1.55911369e+01 -3.86822739e+01  3.05911369e+01  5.36822739e+01\n",
      "   2.31495016e+01  1.87718683e+00  1.87127420e+00  3.86924541e+00\n",
      "   1.04656997e+00]\n",
      " [-3.86822739e+01 -3.86822739e+01  5.36822739e+01  5.36822739e+01\n",
      "   2.28415598e+01  1.88597526e+00  1.87818766e+00  3.89712243e+00\n",
      "   1.03417498e+00]\n",
      " [-6.17734108e+01 -3.86822739e+01  7.67734070e+01  5.36822739e+01\n",
      "   2.23343746e+01  1.87899723e+00  1.88468885e+00  3.92531731e+00\n",
      "   1.02817039e+00]\n",
      " [-2.14436092e+01 -5.03872185e+01  3.64436111e+01  6.53872223e+01\n",
      "   2.22342256e+01  1.87983632e+00  1.87899495e+00  3.91124522e+00\n",
      "   1.02954711e+00]\n",
      " [-5.03872185e+01 -5.03872185e+01  6.53872223e+01  6.53872223e+01\n",
      "   2.16313681e+01  1.87453613e+00  1.88258537e+00  3.92106698e+00\n",
      "   1.00079029e+00]\n",
      " [-7.93308334e+01 -5.03872185e+01  9.43308334e+01  6.53872223e+01\n",
      "   2.12589314e+01  1.86909311e+00  1.88691145e+00  3.93603023e+00\n",
      "   9.87236141e-01]\n",
      " [-2.87793999e+01 -6.50587997e+01  4.37793999e+01  8.00587997e+01\n",
      "   2.12074443e+01  1.87015055e+00  1.88545235e+00  3.93489703e+00\n",
      "   9.88116831e-01]\n",
      " [-6.50587997e+01 -6.50587997e+01  8.00587997e+01  8.00587997e+01\n",
      "   2.09273698e+01  1.87267696e+00  1.88968850e+00  3.95002642e+00\n",
      "   9.75121377e-01]\n",
      " [-1.01338196e+02 -6.50587997e+01  1.16338196e+02  8.00587997e+01\n",
      "   2.08994899e+01  1.87315300e+00  1.89061312e+00  3.95760716e+00\n",
      "   9.76222338e-01]\n",
      " [-3.79744530e+01 -8.34489059e+01  5.29744530e+01  9.84489059e+01\n",
      "   2.08922793e+01  1.87340358e+00  1.89066824e+00  3.95810758e+00\n",
      "   9.75736866e-01]\n",
      " [-8.34489059e+01 -8.34489059e+01  9.84489059e+01  9.84489059e+01\n",
      "   2.08365595e+01  1.87951468e+00  1.89504481e+00  3.97728388e+00\n",
      "   9.73609577e-01]\n",
      " [-1.28923355e+02 -8.34489059e+01  1.43923355e+02  9.84489059e+01\n",
      "   2.08044213e+01  1.88098872e+00  1.89629538e+00  3.98640127e+00\n",
      "   9.72883490e-01]\n",
      " [-4.95000000e+01 -1.06500000e+02  6.45000000e+01  1.21500000e+02\n",
      "   2.07815741e+01  1.88311556e+00  1.89693522e+00  3.99174032e+00\n",
      "   9.72332004e-01]\n",
      " [-1.06500000e+02 -1.06500000e+02  1.21500000e+02  1.21500000e+02\n",
      "   2.07093998e+01  1.89172212e+00  1.90250205e+00  4.01958549e+00\n",
      "   9.68637147e-01]\n",
      " [-1.63500000e+02 -1.06500000e+02  1.78500000e+02  1.21500000e+02\n",
      "   2.06990382e+01  1.89438241e+00  1.90411548e+00  4.02852785e+00\n",
      "   9.69535749e-01]]\n",
      "[[0.13403545 0.12899113 0.24656938 0.24187183 0.29868612 0.20564263\n",
      "  6.18805623 0.30421349 0.29736375 0.44455876 1.17853777]]\n",
      "[[ 5.24335248e-04  2.06723704e-05  4.34140272e-03 -5.49570152e-02\n",
      "  -6.07585096e-02  3.73098442e-02 -1.03931329e+01  1.35938426e-02\n",
      "   7.06610783e-02  1.04906628e-01 -3.07511445e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(conf.anchors.shape)\n",
    "print(conf.anchors)\n",
    "print(conf.bbox_stds)\n",
    "print(conf.bbox_means)\n",
    "#print(conf.bbox_stds[:, 4][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95dbb47",
   "metadata": {},
   "source": [
    "## Model forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb2cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate_layer(layer, val):\n",
    "    layer.dilation = val\n",
    "    layer.padding = val\n",
    "\n",
    "\n",
    "class LocalConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, num_rows, num_feats_in, num_feats_out, kernel=1, padding=0):\n",
    "        super(LocalConv2d, self).__init__()\n",
    "\n",
    "        self.num_rows = num_rows\n",
    "        self.out_channels = num_feats_out\n",
    "        self.kernel = kernel\n",
    "        self.pad = padding\n",
    "\n",
    "        self.group_conv = nn.Conv2d(num_feats_in * num_rows, num_feats_out * num_rows, kernel, stride=1, groups=num_rows)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        if self.pad: x = F.pad(x, (self.pad, self.pad, self.pad, self.pad), mode='constant', value=0)\n",
    "\n",
    "        t = int(h / self.num_rows)\n",
    "\n",
    "        # unfold by rows\n",
    "        x = x.unfold(2, t + self.pad*2, t)\n",
    "        x = x.permute([0, 2, 1, 4, 3]).contiguous()\n",
    "        x = x.view(b, c * self.num_rows, t + self.pad*2, (w+self.pad*2)).contiguous()\n",
    "\n",
    "        # group convolution for efficient parallel processing\n",
    "        y = self.group_conv(x)\n",
    "        y = y.view(b, self.num_rows, self.out_channels, t, w).contiguous()\n",
    "        y = y.permute([0, 2, 1, 3, 4]).contiguous()\n",
    "        y = y.view(b, self.out_channels, h, w)\n",
    "\n",
    "        return y\n",
    "def flatten_tensor(input):\n",
    "    \"\"\"\n",
    "    Flattens and permutes a tensor from size\n",
    "    [B x C x W x H] --> [B x (W x H) x C]\n",
    "    \"\"\"\n",
    "\n",
    "    bsize = input.shape[0]\n",
    "    csize = input.shape[1]\n",
    "\n",
    "    return input.permute(0, 2, 3, 1).contiguous().view(bsize, -1, csize)\n",
    "\n",
    "\n",
    "def unflatten_tensor(input, feat_size, anchors):\n",
    "    \"\"\"\n",
    "    Un-flattens and un-permutes a tensor from size\n",
    "    [B x (W x H) x C] --> [B x C x W x H]\n",
    "    \"\"\"\n",
    "\n",
    "    bsize = input.shape[0]\n",
    "\n",
    "    if len(input.shape) >= 3: csize = input.shape[2]\n",
    "    else: csize = 1\n",
    "\n",
    "    input = input.view(bsize, feat_size[0] * anchors.shape[0], feat_size[1], csize)\n",
    "    input = input.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "    return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f431eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "\n",
    "    def __init__(self, phase, base,conf):\n",
    "        super(RPN, self).__init__()\n",
    "\n",
    "        self.base = base\n",
    "\n",
    "        del self.base.transition3.pool\n",
    "\n",
    "        # dilate\n",
    "        dilate_layer(self.base.denseblock4.denselayer1.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer2.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer3.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer4.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer5.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer6.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer7.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer8.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer9.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer10.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer11.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer12.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer13.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer14.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer15.conv2, 2)\n",
    "        dilate_layer(self.base.denseblock4.denselayer16.conv2, 2)\n",
    "\n",
    "        # settings\n",
    "        self.phase = phase\n",
    "        self.num_classes = len(conf['lbls']) + 1\n",
    "        #self.num_classes = len(conf['lbls'])\n",
    "        self.num_anchors = conf['anchors'].shape[0]\n",
    "\n",
    "        self.num_rows = int(min(conf.bins, calc_output_size(conf.test_scale, conf.feat_stride)))\n",
    "        print(self.num_rows)\n",
    "        self.prop_feats = nn.Sequential(\n",
    "            nn.Conv2d(self.base[-1].num_features, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # outputs\n",
    "        self.cls = nn.Conv2d(self.prop_feats[0].out_channels, self.num_classes * self.num_anchors, 1, )\n",
    "\n",
    "        # bbox 2d\n",
    "        self.bbox_x = nn.Conv2d(self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_y = nn.Conv2d(self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_w = nn.Conv2d(self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_h = nn.Conv2d(self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "\n",
    "        # bbox 3d\n",
    "        self.bbox_x3d = nn.Conv2d(self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_y3d = nn.Conv2d(self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_z3d = nn.Conv2d(self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_w3d = nn.Conv2d(self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_h3d = nn.Conv2d(self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_l3d = nn.Conv2d(self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_rY3d = nn.Conv2d(self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "\n",
    "        self.prop_feats_loc = nn.Sequential(\n",
    "            LocalConv2d(self.num_rows, self.base[-1].num_features, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # outputs\n",
    "        self.cls_loc = LocalConv2d(self.num_rows, self.prop_feats[0].out_channels, self.num_classes * self.num_anchors, 1, )\n",
    "\n",
    "        # bbox 2d\n",
    "        self.bbox_x_loc = LocalConv2d(self.num_rows, self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_y_loc = LocalConv2d(self.num_rows, self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_w_loc = LocalConv2d(self.num_rows, self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_h_loc = LocalConv2d(self.num_rows, self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "\n",
    "        # bbox 3d\n",
    "        self.bbox_x3d_loc = LocalConv2d(self.num_rows, self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_y3d_loc = LocalConv2d(self.num_rows, self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_z3d_loc = LocalConv2d(self.num_rows, self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_w3d_loc = LocalConv2d(self.num_rows, self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_h3d_loc = LocalConv2d(self.num_rows, self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_l3d_loc = LocalConv2d(self.num_rows, self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "        self.bbox_rY3d_loc = LocalConv2d(self.num_rows, self.prop_feats[0].out_channels, self.num_anchors, 1)\n",
    "       \n",
    "        self.cls_ble = nn.Parameter(torch.tensor(10e-5).type(torch.FloatTensor))\n",
    "\n",
    "        self.bbox_x_ble = nn.Parameter(torch.tensor(10e-5).type(torch.FloatTensor))\n",
    "        self.bbox_y_ble = nn.Parameter(torch.tensor(10e-5).type(torch.FloatTensor))\n",
    "        self.bbox_w_ble = nn.Parameter(torch.tensor(10e-5).type(torch.FloatTensor))\n",
    "        self.bbox_h_ble = nn.Parameter(torch.tensor(10e-5).type(torch.FloatTensor))\n",
    "\n",
    "        self.bbox_x3d_ble = nn.Parameter(torch.tensor(10e-5).type(torch.FloatTensor))\n",
    "        self.bbox_y3d_ble = nn.Parameter(torch.tensor(10e-5).type(torch.FloatTensor))\n",
    "        self.bbox_z3d_ble = nn.Parameter(torch.tensor(10e-5).type(torch.FloatTensor))\n",
    "        self.bbox_w3d_ble = nn.Parameter(torch.tensor(10e-5).type(torch.FloatTensor))\n",
    "        self.bbox_h3d_ble = nn.Parameter(torch.tensor(10e-5).type(torch.FloatTensor))\n",
    "        self.bbox_l3d_ble = nn.Parameter(torch.tensor(10e-5).type(torch.FloatTensor))\n",
    "        self.bbox_rY3d_ble = nn.Parameter(torch.tensor(10e-5).type(torch.FloatTensor))\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.feat_stride = conf.feat_stride\n",
    "        self.feat_size = calc_output_size(np.array(conf.crop_size), self.feat_stride)\n",
    "        self.rois = locate_anchors(conf.anchors, self.feat_size, conf.feat_stride, convert_tensor=True)\n",
    "        self.rois = self.rois.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        self.anchors = conf.anchors\n",
    "        self.bbox_means = conf.bbox_means\n",
    "        self.bbox_stds = conf.bbox_stds\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # resnet\n",
    "        x = self.base(x)\n",
    "\n",
    "        prop_feats = self.prop_feats(x)\n",
    "        prop_feats_loc = self.prop_feats_loc(x)\n",
    "\n",
    "        cls = self.cls(prop_feats)\n",
    "\n",
    "        # bbox 2d\n",
    "        bbox_x = self.bbox_x(prop_feats)\n",
    "        bbox_y = self.bbox_y(prop_feats)\n",
    "        bbox_w = self.bbox_w(prop_feats)\n",
    "        bbox_h = self.bbox_h(prop_feats)\n",
    "\n",
    "        # bbox 3d\n",
    "        bbox_x3d = self.bbox_x3d(prop_feats)\n",
    "        bbox_y3d = self.bbox_y3d(prop_feats)\n",
    "        bbox_z3d = self.bbox_z3d(prop_feats)\n",
    "        bbox_w3d = self.bbox_w3d(prop_feats)\n",
    "        bbox_h3d = self.bbox_h3d(prop_feats)\n",
    "        bbox_l3d = self.bbox_l3d(prop_feats)\n",
    "        bbox_rY3d = self.bbox_rY3d(prop_feats)\n",
    "\n",
    "        cls_loc = self.cls_loc(prop_feats_loc)\n",
    "\n",
    "        # bbox 2d\n",
    "        bbox_x_loc = self.bbox_x_loc(prop_feats_loc)\n",
    "        bbox_y_loc = self.bbox_y_loc(prop_feats_loc)\n",
    "        bbox_w_loc = self.bbox_w_loc(prop_feats_loc)\n",
    "        bbox_h_loc = self.bbox_h_loc(prop_feats_loc)\n",
    "\n",
    "        # bbox 3d\n",
    "        bbox_x3d_loc = self.bbox_x3d_loc(prop_feats_loc)\n",
    "        bbox_y3d_loc = self.bbox_y3d_loc(prop_feats_loc)\n",
    "        bbox_z3d_loc = self.bbox_z3d_loc(prop_feats_loc)\n",
    "        bbox_w3d_loc = self.bbox_w3d_loc(prop_feats_loc)\n",
    "        bbox_h3d_loc = self.bbox_h3d_loc(prop_feats_loc)\n",
    "        bbox_l3d_loc = self.bbox_l3d_loc(prop_feats_loc)\n",
    "        bbox_rY3d_loc = self.bbox_rY3d_loc(prop_feats_loc)\n",
    "\n",
    "        cls_ble = self.sigmoid(self.cls_ble)\n",
    "\n",
    "        # bbox 2d\n",
    "        bbox_x_ble = self.sigmoid(self.bbox_x_ble)\n",
    "        bbox_y_ble = self.sigmoid(self.bbox_y_ble)\n",
    "        bbox_w_ble = self.sigmoid(self.bbox_w_ble)\n",
    "        bbox_h_ble = self.sigmoid(self.bbox_h_ble)\n",
    "\n",
    "        # bbox 3d\n",
    "        bbox_x3d_ble = self.sigmoid(self.bbox_x3d_ble)\n",
    "        bbox_y3d_ble = self.sigmoid(self.bbox_y3d_ble)\n",
    "        bbox_z3d_ble = self.sigmoid(self.bbox_z3d_ble)\n",
    "        bbox_w3d_ble = self.sigmoid(self.bbox_w3d_ble)\n",
    "        bbox_h3d_ble = self.sigmoid(self.bbox_h3d_ble)\n",
    "        bbox_l3d_ble = self.sigmoid(self.bbox_l3d_ble)\n",
    "        bbox_rY3d_ble = self.sigmoid(self.bbox_rY3d_ble)\n",
    "\n",
    "        # blend\n",
    "        cls = (cls * cls_ble) + (cls_loc * (1 - cls_ble))\n",
    "\n",
    "        bbox_x = (bbox_x * bbox_x_ble) + (bbox_x_loc * (1 - bbox_x_ble))\n",
    "        bbox_y = (bbox_y * bbox_y_ble) + (bbox_y_loc * (1 - bbox_y_ble))\n",
    "        bbox_w = (bbox_w * bbox_w_ble) + (bbox_w_loc * (1 - bbox_w_ble))\n",
    "        bbox_h = (bbox_h * bbox_h_ble) + (bbox_h_loc * (1 - bbox_h_ble))\n",
    "\n",
    "        bbox_x3d = (bbox_x3d * bbox_x3d_ble) + (bbox_x3d_loc * (1 - bbox_x3d_ble))\n",
    "        bbox_y3d = (bbox_y3d * bbox_y3d_ble) + (bbox_y3d_loc * (1 - bbox_y3d_ble))\n",
    "        bbox_z3d = (bbox_z3d * bbox_z3d_ble) + (bbox_z3d_loc * (1 - bbox_z3d_ble))\n",
    "        bbox_h3d = (bbox_h3d * bbox_h3d_ble) + (bbox_h3d_loc * (1 - bbox_h3d_ble))\n",
    "        bbox_w3d = (bbox_w3d * bbox_w3d_ble) + (bbox_w3d_loc * (1 - bbox_w3d_ble))\n",
    "        bbox_l3d = (bbox_l3d * bbox_l3d_ble) + (bbox_l3d_loc * (1 - bbox_l3d_ble))\n",
    "        bbox_rY3d = (bbox_rY3d * bbox_rY3d_ble) + (bbox_rY3d_loc * (1 - bbox_rY3d_ble))\n",
    "\n",
    "        feat_h = cls.size(2)\n",
    "        feat_w = cls.size(3)\n",
    "\n",
    "        # reshape for cross entropy\n",
    "        cls = cls.view(batch_size, self.num_classes, feat_h * self.num_anchors, feat_w)\n",
    "\n",
    "        # score probabilities\n",
    "        prob = self.softmax(cls)\n",
    "\n",
    "        # reshape for consistency\n",
    "        bbox_x = flatten_tensor(bbox_x.view(batch_size, 1, feat_h * self.num_anchors, feat_w))\n",
    "        bbox_y = flatten_tensor(bbox_y.view(batch_size, 1, feat_h * self.num_anchors, feat_w))\n",
    "        bbox_w = flatten_tensor(bbox_w.view(batch_size, 1, feat_h * self.num_anchors, feat_w))\n",
    "        bbox_h = flatten_tensor(bbox_h.view(batch_size, 1, feat_h * self.num_anchors, feat_w))\n",
    "\n",
    "        bbox_x3d = flatten_tensor(bbox_x3d.view(batch_size, 1, feat_h * self.num_anchors, feat_w))\n",
    "        bbox_y3d = flatten_tensor(bbox_y3d.view(batch_size, 1, feat_h * self.num_anchors, feat_w))\n",
    "        bbox_z3d = flatten_tensor(bbox_z3d.view(batch_size, 1, feat_h * self.num_anchors, feat_w))\n",
    "        bbox_w3d = flatten_tensor(bbox_w3d.view(batch_size, 1, feat_h * self.num_anchors, feat_w))\n",
    "        bbox_h3d = flatten_tensor(bbox_h3d.view(batch_size, 1, feat_h * self.num_anchors, feat_w))\n",
    "        bbox_l3d = flatten_tensor(bbox_l3d.view(batch_size, 1, feat_h * self.num_anchors, feat_w))\n",
    "        bbox_rY3d = flatten_tensor(bbox_rY3d.view(batch_size, 1, feat_h * self.num_anchors, feat_w))\n",
    "\n",
    "        # bundle\n",
    "        bbox_2d = torch.cat((bbox_x, bbox_y, bbox_w, bbox_h), dim=2)\n",
    "        bbox_3d = torch.cat((bbox_x3d, bbox_y3d, bbox_z3d, bbox_w3d, bbox_h3d, bbox_l3d, bbox_rY3d), dim=2)\n",
    "\n",
    "        feat_size = [feat_h, feat_w]\n",
    "\n",
    "        cls = flatten_tensor(cls)\n",
    "        prob = flatten_tensor(prob)\n",
    "\n",
    "        if self.feat_size[0] != feat_h or self.feat_size[1] != feat_w:\n",
    "            self.feat_size = [feat_h, feat_w]\n",
    "            self.rois = locate_anchors(self.anchors, self.feat_size, self.feat_stride, convert_tensor=True)\n",
    "            self.rois = self.rois.type(torch.cuda.FloatTensor)\n",
    "            #self.rois = self.rois.type(torch.FloatTensor)\n",
    "        if self.training:\n",
    "            return cls, prob, bbox_2d, bbox_3d, feat_size\n",
    "\n",
    "        else:\n",
    "            return cls, prob, bbox_2d, bbox_3d, feat_size, self.rois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba68d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device= torch.device('cuda:2')\n",
    "else:\n",
    "    device= torch.device('cpu')\n",
    "phase = 'train'\n",
    "train = phase.lower() == 'train'\n",
    "densenet121 = models.densenet121(pretrained=train)\n",
    "densenet121= densenet121.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e487a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(densenet121.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40629b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_net = RPN(phase, densenet121.features, conf)\n",
    "#PATH = r\"E:\\Rishav_Thesis\\Baseline\\Monocular_3D_Object_Det\\ordered\\Saved_model\\model_parameter_84.pth\"\n",
    "#rpn_net.load_state_dict(torch.load(PATH))\n",
    "rpn_net= rpn_net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9462bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = conf.lr\n",
    "wd = conf.weight_decay\n",
    "optimizer = torch.optim.Adam(rpn_net.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc8aa31",
   "metadata": {},
   "source": [
    "## SetUp Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea98d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_transform_inv(boxes, deltas, means=None, stds=None):\n",
    "    \"\"\"\n",
    "    Compute the bbox target transforms in 3D.\n",
    "    Translations are done as simple difference, whereas others involving\n",
    "    scaling are done in log space (hence, log(1) = 0, log(0.8) < 0 and\n",
    "    log(1.2) > 0 which is a good property).\n",
    "    \"\"\"\n",
    "    \n",
    "    if boxes.shape[0] == 0:\n",
    "        return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n",
    "\n",
    "    # boxes = boxes.astype(deltas.dtype, copy=False)\n",
    "\n",
    "    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n",
    "    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n",
    "    ctr_x = boxes[:, 0] + 0.5 * widths\n",
    "    ctr_y = boxes[:, 1] + 0.5 * heights\n",
    "\n",
    "    dx = deltas[:, 0]\n",
    "    dy = deltas[:, 1] \n",
    "    dw = deltas[:, 2]\n",
    "    dh = deltas[:, 3]\n",
    "\n",
    "    if stds is not None:\n",
    "        dx *= stds[0]\n",
    "        dy *= stds[1]\n",
    "        dw *= stds[2]\n",
    "        dh *= stds[3]\n",
    "\n",
    "    if means is not None:\n",
    "        dx += means[0]\n",
    "        dy += means[1]\n",
    "        dw += means[2]\n",
    "        dh += means[3]\n",
    "\n",
    "    pred_ctr_x = dx * widths + ctr_x\n",
    "    pred_ctr_y = dy * heights + ctr_y\n",
    "    pred_w = torch.exp(dw) * widths\n",
    "    pred_h = torch.exp(dh) * heights\n",
    "\n",
    "    pred_boxes = torch.zeros(deltas.shape)\n",
    "\n",
    "    # x1, y1, x2, y2\n",
    "    pred_boxes[:, 0] = pred_ctr_x - 0.5 * pred_w\n",
    "    pred_boxes[:, 1] = pred_ctr_y - 0.5 * pred_h\n",
    "    pred_boxes[:, 2] = pred_ctr_x + 0.5 * pred_w\n",
    "    pred_boxes[:, 3] = pred_ctr_y + 0.5 * pred_h\n",
    "\n",
    "    return pred_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ac420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN_3D_loss(nn.Module):\n",
    "\n",
    "    def __init__(self, conf):\n",
    "\n",
    "        super(RPN_3D_loss, self).__init__()\n",
    "\n",
    "        self.num_classes = len(conf.lbls) + 1\n",
    "        self.num_anchors = conf.anchors.shape[0]\n",
    "        self.anchors = conf.anchors\n",
    "        self.bbox_means = conf.bbox_means\n",
    "        self.bbox_stds = conf.bbox_stds\n",
    "        self.feat_stride = conf.feat_stride\n",
    "        self.fg_fraction = conf.fg_fraction\n",
    "        self.box_samples = conf.box_samples\n",
    "        self.ign_thresh = conf.ign_thresh\n",
    "        self.nms_thres = conf.nms_thres\n",
    "        self.fg_thresh = conf.fg_thresh\n",
    "        self.bg_thresh_lo = conf.bg_thresh_lo\n",
    "        self.bg_thresh_hi = conf.bg_thresh_hi\n",
    "        self.best_thresh = conf.best_thresh\n",
    "        self.hard_negatives = conf.hard_negatives\n",
    "        self.focal_loss = conf.focal_loss\n",
    "\n",
    "        self.crop_size = conf.crop_size\n",
    "\n",
    "        self.cls_2d_lambda = conf.cls_2d_lambda\n",
    "        self.iou_2d_lambda = conf.iou_2d_lambda\n",
    "        self.bbox_2d_lambda = conf.bbox_2d_lambda\n",
    "        self.bbox_3d_lambda = conf.bbox_3d_lambda\n",
    "        self.bbox_3d_proj_lambda = conf.bbox_3d_proj_lambda\n",
    "\n",
    "        self.lbls = conf.lbls\n",
    "        self.ilbls = conf.ilbls\n",
    "\n",
    "        self.min_gt_vis = conf.min_gt_vis\n",
    "        self.min_gt_h = conf.min_gt_h\n",
    "        self.max_gt_h = conf.max_gt_h\n",
    "        self.device = 'cuda:2'\n",
    "\n",
    "    def forward(self, cls, prob, bbox_2d, bbox_3d, imobjs, feat_size):\n",
    "\n",
    "        stats = []\n",
    "        \n",
    "        loss = torch.tensor(0, device= self.device).type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        FG_ENC = 1000\n",
    "        BG_ENC = 2000\n",
    "\n",
    "        IGN_FLAG = 3000\n",
    "\n",
    "        batch_size = cls.shape[0]\n",
    "\n",
    "        #prob_detach = prob.cpu().detach().numpy()\n",
    "        prob_detach = prob.cpu().detach().numpy()\n",
    "        \n",
    "        bbox_x = bbox_2d[:, :, 0]\n",
    "        bbox_y = bbox_2d[:, :, 1]\n",
    "        bbox_w = bbox_2d[:, :, 2]\n",
    "        bbox_h = bbox_2d[:, :, 3]\n",
    "\n",
    "        bbox_x3d = bbox_3d[:, :, 0]\n",
    "        bbox_y3d = bbox_3d[:, :, 1]\n",
    "        bbox_z3d = bbox_3d[:, :, 2]\n",
    "        bbox_w3d = bbox_3d[:, :, 3]\n",
    "        bbox_h3d = bbox_3d[:, :, 4]\n",
    "        bbox_l3d = bbox_3d[:, :, 5]\n",
    "        bbox_ry3d = bbox_3d[:, :, 6]\n",
    "\n",
    "        bbox_x3d_proj = torch.zeros(bbox_x3d.shape, device= self.device)\n",
    "        bbox_y3d_proj = torch.zeros(bbox_x3d.shape, device= self.device)\n",
    "        bbox_z3d_proj = torch.zeros(bbox_x3d.shape, device= self.device)\n",
    "\n",
    "        labels = np.zeros(cls.shape[0:2])\n",
    "        labels_weight = np.zeros(cls.shape[0:2])\n",
    "\n",
    "        labels_scores = np.zeros(cls.shape[0:2])\n",
    "\n",
    "        bbox_x_tar = np.zeros(cls.shape[0:2])\n",
    "        bbox_y_tar = np.zeros(cls.shape[0:2])\n",
    "        bbox_w_tar = np.zeros(cls.shape[0:2])\n",
    "        bbox_h_tar = np.zeros(cls.shape[0:2])\n",
    "\n",
    "        bbox_x3d_tar = np.zeros(cls.shape[0:2])\n",
    "        bbox_y3d_tar = np.zeros(cls.shape[0:2])\n",
    "        bbox_z3d_tar = np.zeros(cls.shape[0:2])\n",
    "        bbox_w3d_tar = np.zeros(cls.shape[0:2])\n",
    "        bbox_h3d_tar = np.zeros(cls.shape[0:2])\n",
    "        bbox_l3d_tar = np.zeros(cls.shape[0:2])\n",
    "        bbox_ry3d_tar = np.zeros(cls.shape[0:2])\n",
    "\n",
    "        bbox_x3d_proj_tar = np.zeros(cls.shape[0:2])\n",
    "        bbox_y3d_proj_tar = np.zeros(cls.shape[0:2])\n",
    "        bbox_z3d_proj_tar = np.zeros(cls.shape[0:2])\n",
    "\n",
    "        bbox_weights = np.zeros(cls.shape[0:2])\n",
    "\n",
    "        ious_2d = torch.zeros(cls.shape[0:2])\n",
    "        ious_3d = torch.zeros(cls.shape[0:2])\n",
    "        coords_abs_z = torch.zeros(cls.shape[0:2], device= self.device)\n",
    "        coords_abs_ry = torch.zeros(cls.shape[0:2], device= self.device)\n",
    "\n",
    "        # get all rois\n",
    "        rois = locate_anchors(self.anchors, feat_size, self.feat_stride, convert_tensor=True)\n",
    "        rois = torch.tensor(rois, device= self.device).type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        bbox_x3d_dn = bbox_x3d * self.bbox_stds[:, 4][0] + self.bbox_means[:, 4][0]\n",
    "        bbox_y3d_dn = bbox_y3d * self.bbox_stds[:, 5][0] + self.bbox_means[:, 5][0]\n",
    "        bbox_z3d_dn = bbox_z3d * self.bbox_stds[:, 6][0] + self.bbox_means[:, 6][0]\n",
    "        bbox_w3d_dn = bbox_w3d * self.bbox_stds[:, 7][0] + self.bbox_means[:, 7][0]\n",
    "        bbox_h3d_dn = bbox_h3d * self.bbox_stds[:, 8][0] + self.bbox_means[:, 8][0]\n",
    "        bbox_l3d_dn = bbox_l3d * self.bbox_stds[:, 9][0] + self.bbox_means[:, 9][0]\n",
    "        bbox_ry3d_dn = bbox_ry3d * self.bbox_stds[:, 10][0] + self.bbox_means[:, 10][0]\n",
    "\n",
    "        src_anchors = self.anchors[rois[:, 4].type(torch.cuda.LongTensor).cpu(), :]\n",
    "        \n",
    "        src_anchors = torch.tensor(src_anchors,device= self.device, requires_grad=False).type(torch.cuda.FloatTensor)\n",
    "        if len(src_anchors.shape) == 1: src_anchors = src_anchors.unsqueeze(0)\n",
    "\n",
    "        # compute 3d transform\n",
    "        widths = rois[:, 2] - rois[:, 0] + 1.0\n",
    "        heights = rois[:, 3] - rois[:, 1] + 1.0\n",
    "        ctr_x = rois[:, 0] + 0.5 * widths\n",
    "        ctr_y = rois[:, 1] + 0.5 * heights\n",
    "\n",
    "        bbox_x3d_dn = bbox_x3d_dn * widths.unsqueeze(0) + ctr_x.unsqueeze(0)\n",
    "        bbox_y3d_dn = bbox_y3d_dn * heights.unsqueeze(0) + ctr_y.unsqueeze(0)\n",
    "        bbox_z3d_dn = src_anchors[:, 4].unsqueeze(0) + bbox_z3d_dn\n",
    "        bbox_w3d_dn = torch.exp(bbox_w3d_dn) * src_anchors[:, 5].unsqueeze(0)\n",
    "        bbox_h3d_dn = torch.exp(bbox_h3d_dn) * src_anchors[:, 6].unsqueeze(0)\n",
    "        bbox_l3d_dn = torch.exp(bbox_l3d_dn) * src_anchors[:, 7].unsqueeze(0)\n",
    "        bbox_ry3d_dn = src_anchors[:, 8].unsqueeze(0) + bbox_ry3d_dn\n",
    "\n",
    "        for bind in range(0, batch_size):\n",
    "\n",
    "            #imobj = imobjs[bind]\n",
    "            imobj = imobjs\n",
    "            gts = imobj.gts\n",
    "            imobj.imH= 1216\n",
    "            imobj.imW= 1920\n",
    "            imobj.scale = 1\n",
    "            \n",
    "            if len(imobj.gts) > 0:\n",
    "                scale_factor = imobj.scale * conf.test_scale / imobj.imH\n",
    "\n",
    "                imobj.scale_factor= scale_factor\n",
    "\n",
    "            p2_inv = torch.tensor(imobj.p2_inv, device= self.device).type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            # filter gts\n",
    "            igns, rmvs = determine_ignores(gts, self.lbls, self.ilbls, self.min_gt_vis, self.min_gt_h)\n",
    "\n",
    "            # accumulate boxes\n",
    "\n",
    "            scaled_gts= np.empty((len(imobj.gts),4))\n",
    "            j=0 \n",
    "            while (j <(len(imobj.gts))):\n",
    "                for gt in imobj.gts:\n",
    "                    for i in range(4):\n",
    "                        scaled_gts[j,i]= gt.bbox_full[i]*scale_factor\n",
    "                    j= j+1\n",
    "            #print(np.shape(scaled_gts))\n",
    "            gts_all =  bbXYWH2Coords(scaled_gts)\n",
    "            gts_3d = np.array([gt.bbox_3d for gt in gts])\n",
    "            \n",
    "            for gtind, gt in enumerate(gts_3d):\n",
    "                gts_3d[gtind, 0:2] *= scale_factor\n",
    "\n",
    "            if not ((rmvs == False) & (igns == False)).any():\n",
    "                continue\n",
    "\n",
    "            # filter out irrelevant cls, and ignore cls\n",
    "            gts_val = gts_all[(rmvs == False) & (igns == False), :]\n",
    "            gts_ign = gts_all[(rmvs == False) & (igns == True), :]\n",
    "            gts_3d = gts_3d[(rmvs == False) & (igns == False), :]\n",
    "\n",
    "            # accumulate labels\n",
    "            box_lbls = np.array([gt.cls[0] for gt in gts])\n",
    "            box_lbls = box_lbls[(rmvs == False) & (igns == False)]\n",
    "            box_lbls = np.array([clsName2Ind(self.lbls, cls) for cls in box_lbls])\n",
    "\n",
    "            if gts_val.shape[0] > 0 or gts_ign.shape[0] > 0:\n",
    "\n",
    "                rois = rois.cpu()\n",
    "\n",
    "                # bbox regression\n",
    "                transforms, ols, raw_gt = compute_targets(gts_val, gts_ign, box_lbls, rois.numpy(), self.fg_thresh,\n",
    "                                                  self.ign_thresh, self.bg_thresh_lo, self.bg_thresh_hi,\n",
    "                                                  self.best_thresh, anchors=self.anchors,  gts_3d=gts_3d,\n",
    "                                                  tracker=rois[:, 4].numpy())\n",
    "      \n",
    "                # normalize 2d\n",
    "                transforms[:, 0:4] -= self.bbox_means[:, 0:4]\n",
    "                transforms[:, 0:4] /= self.bbox_stds[:, 0:4]\n",
    "\n",
    "                # normalize 3d\n",
    "                transforms[:, 5:12] -= self.bbox_means[:, 4:]\n",
    "                transforms[:, 5:12] /= self.bbox_stds[:, 4:]\n",
    "\n",
    "                labels_fg = transforms[:, 4] > 0\n",
    "                labels_bg = transforms[:, 4] < 0\n",
    "                labels_ign = transforms[:, 4] == 0\n",
    "\n",
    "                fg_inds = np.flatnonzero(labels_fg)\n",
    "                bg_inds = np.flatnonzero(labels_bg)\n",
    "                ign_inds = np.flatnonzero(labels_ign)\n",
    "                #print(\"fg_inds\",fg_inds)\n",
    "                #print(\"bg_inds\",fg_inds)\n",
    "                #print(\"ign_inds\",fg_inds)\n",
    "                #transforms = torch.tensor(np.array(transforms), device= self.device)\n",
    "                \n",
    "                labels[bind,fg_inds] = transforms[fg_inds, 4]\n",
    "                labels[bind, ign_inds] = IGN_FLAG\n",
    "                labels[bind, bg_inds] = 0\n",
    "\n",
    "                bbox_x_tar[bind, :] = transforms[:, 0]\n",
    "                bbox_y_tar[bind, :] = transforms[:, 1]\n",
    "                bbox_w_tar[bind, :] = transforms[:, 2]\n",
    "                bbox_h_tar[bind, :] = transforms[:, 3]\n",
    "\n",
    "                bbox_x3d_tar[bind, :] = transforms[:, 5]\n",
    "                bbox_y3d_tar[bind,:] = transforms[:, 6]\n",
    "                bbox_z3d_tar[bind,:] = transforms[:, 7]\n",
    "                bbox_w3d_tar[bind, :] = transforms[:, 8]\n",
    "                bbox_h3d_tar[bind, :] = transforms[:, 9]\n",
    "                bbox_l3d_tar[bind, :] = transforms[:, 10]\n",
    "                bbox_ry3d_tar[bind, :] = transforms[:, 11]\n",
    "\n",
    "                bbox_x3d_proj_tar[bind, :] = raw_gt[:, 12]\n",
    "                bbox_y3d_proj_tar[bind, :] = raw_gt[:, 13]\n",
    "                bbox_z3d_proj_tar[bind, :] = raw_gt[:, 14]\n",
    "\n",
    "                # ----------------------------------------\n",
    "                # box sampling\n",
    "                # ----------------------------------------\n",
    "\n",
    "                if self.box_samples == np.inf:\n",
    "                    fg_num = len(fg_inds)\n",
    "                    bg_num = len(bg_inds)\n",
    "\n",
    "                else:\n",
    "                    fg_num = min(round(rois.shape[0]*self.box_samples * self.fg_fraction), len(fg_inds))\n",
    "                    bg_num = min(round(rois.shape[0]*self.box_samples - fg_num), len(bg_inds))\n",
    "                \n",
    "                if self.hard_negatives:\n",
    "\n",
    "                    if fg_num > 0 and fg_num != fg_inds.shape[0]:\n",
    "                        scores = prob_detach[bind, fg_inds, labels[bind, fg_inds].astype(int)]\n",
    "                        fg_score_ascend = (scores).argsort()\n",
    "                        fg_inds = fg_inds[fg_score_ascend]\n",
    "                        fg_inds = fg_inds[0:fg_num]\n",
    "\n",
    "                    if bg_num > 0 and bg_num != bg_inds.shape[0]:\n",
    "                        scores = prob_detach[bind,bg_inds, labels[bind, bg_inds].astype(int)]\n",
    "                        bg_score_ascend = (scores).argsort()\n",
    "                        bg_inds = bg_inds[bg_score_ascend]\n",
    "                        bg_inds = bg_inds[0:bg_num]\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if fg_num > 0 and fg_num != fg_inds.shape[0]:\n",
    "                        fg_inds = np.random.choice(fg_inds, fg_num, replace=False)\n",
    "\n",
    "                    if bg_num > 0 and bg_num != bg_inds.shape[0]:\n",
    "                        bg_inds = np.random.choice(bg_inds, bg_num, replace=False)\n",
    "\n",
    "                labels_weight[bind, bg_inds] = BG_ENC\n",
    "                labels_weight[bind, fg_inds] = FG_ENC\n",
    "                bbox_weights[bind, fg_inds] = 1\n",
    "\n",
    "                # ----------------------------------------\n",
    "                # compute IoU stats\n",
    "                # ----------------------------------------\n",
    " \n",
    "                if fg_num > 0:\n",
    "\n",
    "                    # compile deltas pred\n",
    "                    deltas_2d = torch.cat((bbox_x[bind, :, np.newaxis ], bbox_y[bind, :, np.newaxis],\n",
    "                                           bbox_w[bind, :, np.newaxis], bbox_h[bind, :, np.newaxis]), dim=1)\n",
    "\n",
    "                    # compile deltas targets\n",
    "                    deltas_2d_tar = np.concatenate((bbox_x_tar[bind,:, np.newaxis], bbox_y_tar[bind, :, np.newaxis],\n",
    "                                                    bbox_w_tar[bind, :, np.newaxis], bbox_h_tar[bind, :, np.newaxis]),\n",
    "                                                   axis=1)\n",
    "\n",
    "                    # move to gpu\n",
    "                    deltas_2d_tar = torch.tensor(deltas_2d_tar, device= self.device,requires_grad=False).type(torch.cuda.FloatTensor)\n",
    "                    \n",
    "                    means = self.bbox_means[0, :]\n",
    "                    stds = self.bbox_stds[0, :]\n",
    "\n",
    "                    rois = rois.to(device)\n",
    "\n",
    "                    coords_2d = bbox_transform_inv(rois, deltas_2d, means=means, stds=stds)\n",
    "                    coords_2d_tar = bbox_transform_inv(rois, deltas_2d_tar, means=means, stds=stds)\n",
    "\n",
    "                    ious_2d[bind,fg_inds] = iou(coords_2d[fg_inds, :], coords_2d_tar[fg_inds, :], mode='list')\n",
    "\n",
    "                    bbox_x3d_dn_fg = bbox_x3d_dn[bind, fg_inds]\n",
    "                    bbox_y3d_dn_fg = bbox_y3d_dn[bind,fg_inds]\n",
    "\n",
    "                    src_anchors = self.anchors[rois[fg_inds, 4].type(torch.LongTensor), :]\n",
    "                    src_anchors = torch.tensor(src_anchors, device= self.device, requires_grad=False).type(torch.cuda.FloatTensor)\n",
    "                    if len(src_anchors.shape) == 1: src_anchors = src_anchors.unsqueeze(0)\n",
    "\n",
    "                    bbox_x3d_dn_fg = bbox_x3d_dn[bind,fg_inds]\n",
    "                    bbox_y3d_dn_fg = bbox_y3d_dn[bind, fg_inds]\n",
    "                    bbox_z3d_dn_fg = bbox_z3d_dn[bind, fg_inds]\n",
    "                    bbox_w3d_dn_fg = bbox_w3d_dn[bind, fg_inds]\n",
    "                    bbox_h3d_dn_fg = bbox_h3d_dn[bind, fg_inds]\n",
    "                    bbox_l3d_dn_fg = bbox_l3d_dn[bind, fg_inds]\n",
    "                    bbox_ry3d_dn_fg = bbox_ry3d_dn[bind, fg_inds]\n",
    "\n",
    "                    # re-scale all 2D back to original\n",
    "                    bbox_x3d_dn_fg /= imobj['scale_factor']\n",
    "                    bbox_y3d_dn_fg /= imobj['scale_factor']\n",
    "\n",
    "                    coords_2d = torch.cat((bbox_x3d_dn_fg[np.newaxis,:] * bbox_z3d_dn_fg[np.newaxis,:], bbox_y3d_dn_fg[np.newaxis,:] * bbox_z3d_dn_fg[np.newaxis,:], bbox_z3d_dn_fg[np.newaxis,:]), dim=0)\n",
    "                    \n",
    "                    coords_2d = torch.cat((coords_2d, (torch.ones([1, coords_2d.shape[1]]).to(device))), dim=0)\n",
    "                    #print(\"coords_2d\",coords_2d.size())\n",
    "                    coords_3d = torch.squeeze((torch.matmul(p2_inv, coords_2d)), dim= 0)\n",
    "                    #print(\"coords_3d\",coords_3d.size())\n",
    "                    #print(\"bbox_x3d_proj\",bbox_x3d_proj.size())\n",
    "                    bbox_x3d_proj[bind,fg_inds] = coords_3d[0, :]\n",
    "                    bbox_y3d_proj[bind,fg_inds] = coords_3d[1, :]\n",
    "                    bbox_z3d_proj[bind,fg_inds] = coords_3d[2, :]\n",
    "\n",
    "                    # absolute targets\n",
    "                    bbox_z3d_dn_tar = bbox_z3d_tar[bind, fg_inds] * self.bbox_stds[:, 6][0] + self.bbox_means[:, 6][0]\n",
    "                    bbox_z3d_dn_tar = torch.tensor(bbox_z3d_dn_tar, device= self.device, requires_grad=False).type(torch.cuda.FloatTensor)\n",
    "                    bbox_z3d_dn_tar = src_anchors[:, 4] + bbox_z3d_dn_tar\n",
    "\n",
    "                    bbox_ry3d_dn_tar = bbox_ry3d_tar[bind, fg_inds] * self.bbox_stds[:, 10][0] + self.bbox_means[:, 10][0]\n",
    "                    bbox_ry3d_dn_tar = torch.tensor(bbox_ry3d_dn_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor)\n",
    "                    bbox_ry3d_dn_tar = src_anchors[:, 8] + bbox_ry3d_dn_tar\n",
    "\n",
    "                    coords_abs_z[bind,fg_inds] = torch.abs(bbox_z3d_dn_tar - bbox_z3d_dn_fg)\n",
    "                    coords_abs_ry[bind,fg_inds] = torch.abs(bbox_ry3d_dn_tar - bbox_ry3d_dn_fg)\n",
    "                \n",
    "            else:\n",
    "\n",
    "                bg_inds = np.arange(0, rois.shape[0])\n",
    "\n",
    "                if self.box_samples == np.inf: bg_num = len(bg_inds)\n",
    "                else: bg_num = min(round(self.box_samples * (1 - self.fg_fraction)), len(bg_inds))\n",
    "\n",
    "                if self.hard_negatives:\n",
    "\n",
    "                    if bg_num > 0 and bg_num != bg_inds.shape[0]:\n",
    "                        scores = prob_detach[bind, bg_inds, labels[bind, bg_inds].astype(int)]\n",
    "                        bg_score_ascend = (scores).argsort()\n",
    "                        bg_inds = bg_inds[bg_score_ascend]\n",
    "                        bg_inds = bg_inds[0:bg_num]\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if bg_num > 0 and bg_num != bg_inds.shape[0]:\n",
    "                        bg_inds = np.random.choice(bg_inds, bg_num, replace=False)\n",
    "\n",
    "\n",
    "                labels[bind,:] = 0\n",
    "                labels_weight[bind,bg_inds] = BG_ENC\n",
    "\n",
    "\n",
    "            # grab label predictions (for weighing purposes)\n",
    "            active = labels[bind, :] != IGN_FLAG\n",
    "            labels_scores[bind,active] = prob_detach[bind,active, labels[bind,active].astype(int)]\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # useful statistics\n",
    "        # ----------------------------------------\n",
    "        \n",
    "        fg_inds_all = np.flatnonzero((labels > 0) & (labels != IGN_FLAG))\n",
    "        bg_inds_all = np.flatnonzero((labels == 0) & (labels != IGN_FLAG))\n",
    "\n",
    "        fg_inds_unravel = np.unravel_index(fg_inds_all, prob_detach.shape[0:2])\n",
    "        bg_inds_unravel = np.unravel_index(bg_inds_all, prob_detach.shape[0:2])\n",
    "\n",
    "        cls_pred = cls.argmax(dim=2).cpu().detach().numpy()\n",
    "\n",
    "        if self.cls_2d_lambda and len(fg_inds_all) > 0:\n",
    "            acc_fg = np.mean(cls_pred[fg_inds_unravel] == labels[fg_inds_unravel])\n",
    "            stats.append({'name': 'fg', 'val': acc_fg, 'format': '{:0.2f}', 'group': 'acc'})\n",
    "            #writer.add_scalar(\"fg_accuracy_train\",acc_fg)\n",
    "        if self.cls_2d_lambda and len(bg_inds_all) > 0:\n",
    "            acc_bg = np.mean(cls_pred[bg_inds_unravel] == labels[bg_inds_unravel])\n",
    "            stats.append({'name': 'bg', 'val': acc_bg, 'format': '{:0.2f}', 'group': 'acc'})\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # box weighting\n",
    "        # ----------------------------------------\n",
    "\n",
    "        fg_inds = np.flatnonzero(labels_weight == FG_ENC)\n",
    "        bg_inds = np.flatnonzero(labels_weight == BG_ENC)\n",
    "        active_inds = np.concatenate((fg_inds, bg_inds), axis=0)\n",
    "\n",
    "        fg_num = len(fg_inds)\n",
    "        bg_num = len(bg_inds)\n",
    "\n",
    "        labels_weight[...] = 0.0\n",
    "        box_samples = fg_num + bg_num\n",
    "\n",
    "        fg_inds_unravel = np.unravel_index(fg_inds, labels_weight.shape)\n",
    "        bg_inds_unravel = np.unravel_index(bg_inds, labels_weight.shape)\n",
    "        active_inds_unravel = np.unravel_index(active_inds, labels_weight.shape)\n",
    "\n",
    "        labels_weight[active_inds_unravel] = 1.0\n",
    "\n",
    "        if self.fg_fraction is not None:\n",
    "\n",
    "            if fg_num > 0:\n",
    "\n",
    "                fg_weight = (self.fg_fraction /(1 - self.fg_fraction)) * (bg_num / fg_num)\n",
    "                labels_weight[fg_inds_unravel] = fg_weight\n",
    "                labels_weight[bg_inds_unravel] = 1.0\n",
    "\n",
    "            else:\n",
    "                labels_weight[bg_inds_unravel] = 1.0\n",
    "\n",
    "        # different method of doing hard negative mining\n",
    "        # use the scores to normalize the importance of each sample\n",
    "        # hence, encourages the network to get all \"correct\" rather than\n",
    "        # becoming more correct at a decision it is already good at\n",
    "        # this method is equivelent to the focal loss with additional mean scaling\n",
    "        if self.focal_loss:\n",
    "\n",
    "            weights_sum = 0\n",
    "\n",
    "            # re-weight bg\n",
    "            if bg_num > 0:\n",
    "                bg_scores = labels_scores[bg_inds_unravel]\n",
    "                bg_weights = (1 - bg_scores) ** self.focal_loss\n",
    "                weights_sum += np.sum(bg_weights)\n",
    "                labels_weight[bg_inds_unravel] *= bg_weights\n",
    "\n",
    "            # re-weight fg\n",
    "            if fg_num > 0:\n",
    "                fg_scores = labels_scores[fg_inds_unravel]\n",
    "                fg_weights = (1 - fg_scores) ** self.focal_loss\n",
    "                weights_sum += np.sum(fg_weights)\n",
    "                labels_weight[fg_inds_unravel] *= fg_weights\n",
    "\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # classification loss\n",
    "        # ----------------------------------------\n",
    "        labels = torch.tensor(labels, requires_grad=False, device= self.device)\n",
    "        labels = labels.view(-1).type(torch.cuda.LongTensor)\n",
    "        \n",
    "        labels_weight = torch.tensor(labels_weight, requires_grad=False, device= self.device)\n",
    "        labels_weight = labels_weight.view(-1).type(torch.cuda.FloatTensor)\n",
    "\n",
    "        cls = cls.view(-1, cls.shape[2])\n",
    "\n",
    "        if self.cls_2d_lambda:\n",
    "\n",
    "            # cls loss\n",
    "            active = labels_weight > 0\n",
    "\n",
    "            if np.any(active.cpu().numpy()):\n",
    "\n",
    "                loss_cls = F.cross_entropy(cls[active, :], labels[active], reduction='none', ignore_index=IGN_FLAG)\n",
    "                loss_cls = (loss_cls * labels_weight[active])\n",
    "\n",
    "                # simple gradient clipping\n",
    "                loss_cls = loss_cls.clamp(min=0, max=2000)\n",
    "\n",
    "                # take mean and scale lambda\n",
    "                loss_cls = loss_cls.mean()\n",
    "                loss_cls *= self.cls_2d_lambda\n",
    "\n",
    "                loss += loss_cls\n",
    "                \n",
    "                stats.append({'name': 'cls', 'val': loss_cls, 'format': '{:0.4f}', 'group': 'loss'})\n",
    "            \n",
    "        # ----------------------------------------\n",
    "        # bbox regression loss\n",
    "        # ----------------------------------------\n",
    "\n",
    "        if np.sum(bbox_weights) > 0:\n",
    "\n",
    "            bbox_weights = torch.tensor(bbox_weights, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "            active = bbox_weights > 0\n",
    "\n",
    "            if self.bbox_2d_lambda:\n",
    "\n",
    "                # bbox loss 2d\n",
    "                bbox_x_tar = torch.tensor(bbox_x_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "                bbox_y_tar = torch.tensor(bbox_y_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "                bbox_w_tar = torch.tensor(bbox_w_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "                bbox_h_tar = torch.tensor(bbox_h_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                bbox_x = bbox_x[:, :].unsqueeze(2).view(-1)\n",
    "                bbox_y = bbox_y[:, :].unsqueeze(2).view(-1)\n",
    "                bbox_w = bbox_w[:, :].unsqueeze(2).view(-1)\n",
    "                bbox_h = bbox_h[:, :].unsqueeze(2).view(-1)\n",
    "\n",
    "                loss_bbox_x = F.smooth_l1_loss(bbox_x[active], bbox_x_tar[active], reduction='none')\n",
    "                loss_bbox_y = F.smooth_l1_loss(bbox_y[active], bbox_y_tar[active], reduction='none')\n",
    "                loss_bbox_w = F.smooth_l1_loss(bbox_w[active], bbox_w_tar[active], reduction='none')\n",
    "                loss_bbox_h = F.smooth_l1_loss(bbox_h[active], bbox_h_tar[active], reduction='none')\n",
    "\n",
    "                loss_bbox_x = (loss_bbox_x * bbox_weights[active]).mean()\n",
    "                loss_bbox_y = (loss_bbox_y * bbox_weights[active]).mean()\n",
    "                loss_bbox_w = (loss_bbox_w * bbox_weights[active]).mean()\n",
    "                loss_bbox_h = (loss_bbox_h * bbox_weights[active]).mean()\n",
    "\n",
    "                bbox_2d_loss = (loss_bbox_x + loss_bbox_y + loss_bbox_w + loss_bbox_h)\n",
    "                bbox_2d_loss *= self.bbox_2d_lambda\n",
    "\n",
    "                loss += bbox_2d_loss\n",
    "                stats.append({'name': 'bbox_2d', 'val': bbox_2d_loss, 'format': '{:0.4f}', 'group': 'loss'})\n",
    "              \n",
    "            if self.bbox_3d_lambda:\n",
    "\n",
    "                # bbox loss 3d\n",
    "                bbox_x3d_tar = torch.tensor(bbox_x3d_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "                bbox_y3d_tar = torch.tensor(bbox_y3d_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "                bbox_z3d_tar = torch.tensor(bbox_z3d_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "                bbox_w3d_tar = torch.tensor(bbox_w3d_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "                bbox_h3d_tar = torch.tensor(bbox_h3d_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "                bbox_l3d_tar = torch.tensor(bbox_l3d_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "                bbox_ry3d_tar = torch.tensor(bbox_ry3d_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "\n",
    "                \n",
    "                \n",
    "                bbox_x3d = bbox_x3d[:, :].view(-1)\n",
    "                bbox_y3d = bbox_y3d[:, :].view(-1)\n",
    "                bbox_z3d = bbox_z3d[:, :].view(-1)\n",
    "                bbox_w3d = bbox_w3d[:, :].view(-1)\n",
    "                bbox_h3d = bbox_h3d[:, :].view(-1)\n",
    "                bbox_l3d = bbox_l3d[:, :].view(-1)\n",
    "                bbox_ry3d = bbox_ry3d[:, :].view(-1)\n",
    "\n",
    "                loss_bbox_x3d = F.smooth_l1_loss(bbox_x3d[active], bbox_x3d_tar[active], reduction='none')\n",
    "                loss_bbox_y3d = F.smooth_l1_loss(bbox_y3d[active], bbox_y3d_tar[active], reduction='none')\n",
    "                loss_bbox_z3d = F.smooth_l1_loss(bbox_z3d[active], bbox_z3d_tar[active], reduction='none')\n",
    "                loss_bbox_w3d = F.smooth_l1_loss(bbox_w3d[active], bbox_w3d_tar[active], reduction='none')\n",
    "                loss_bbox_h3d = F.smooth_l1_loss(bbox_h3d[active], bbox_h3d_tar[active], reduction='none')\n",
    "                loss_bbox_l3d = F.smooth_l1_loss(bbox_l3d[active], bbox_l3d_tar[active], reduction='none')\n",
    "                loss_bbox_ry3d = F.smooth_l1_loss(bbox_ry3d[active], bbox_ry3d_tar[active], reduction='none')\n",
    "\n",
    "                loss_bbox_x3d = (loss_bbox_x3d * bbox_weights[active]).mean()\n",
    "                loss_bbox_y3d = (loss_bbox_y3d * bbox_weights[active]).mean()\n",
    "                loss_bbox_z3d = (loss_bbox_z3d * bbox_weights[active]).mean()\n",
    "                loss_bbox_w3d = (loss_bbox_w3d * bbox_weights[active]).mean()\n",
    "                loss_bbox_h3d = (loss_bbox_h3d * bbox_weights[active]).mean()\n",
    "                loss_bbox_l3d = (loss_bbox_l3d * bbox_weights[active]).mean()\n",
    "                loss_bbox_ry3d = (loss_bbox_ry3d * bbox_weights[active]).mean()\n",
    "\n",
    "                bbox_3d_loss = (loss_bbox_x3d + loss_bbox_y3d + loss_bbox_z3d)\n",
    "                bbox_3d_loss += (loss_bbox_w3d + loss_bbox_h3d + loss_bbox_l3d + loss_bbox_ry3d)\n",
    "\n",
    "                bbox_3d_loss *= self.bbox_3d_lambda\n",
    "\n",
    "                loss += bbox_3d_loss\n",
    "                stats.append({'name': 'bbox_3d', 'val': bbox_3d_loss, 'format': '{:0.4f}', 'group': 'loss'})\n",
    "            \n",
    "            \n",
    "            if self.bbox_3d_proj_lambda:\n",
    "\n",
    "                # bbox loss 3d\n",
    "                bbox_x3d_proj_tar = torch.tensor(bbox_x3d_proj_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "                bbox_y3d_proj_tar = torch.tensor(bbox_y3d_proj_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "                bbox_z3d_proj_tar = torch.tensor(bbox_z3d_proj_tar, requires_grad=False, device= self.device).type(torch.cuda.FloatTensor).view(-1)\n",
    "                \n",
    "                \n",
    "                bbox_x3d_proj = bbox_x3d_proj[:, :].view(-1)\n",
    "                bbox_y3d_proj = bbox_y3d_proj[:, :].view(-1)\n",
    "                bbox_z3d_proj = bbox_z3d_proj[:, :].view(-1)\n",
    "\n",
    "                loss_bbox_x3d_proj = F.smooth_l1_loss(bbox_x3d_proj[active], bbox_x3d_proj_tar[active], reduction='none')\n",
    "                loss_bbox_y3d_proj = F.smooth_l1_loss(bbox_y3d_proj[active], bbox_y3d_proj_tar[active], reduction='none')\n",
    "                loss_bbox_z3d_proj = F.smooth_l1_loss(bbox_z3d_proj[active], bbox_z3d_proj_tar[active], reduction='none')\n",
    "\n",
    "                loss_bbox_x3d_proj = (loss_bbox_x3d_proj * bbox_weights[active]).mean()\n",
    "                loss_bbox_y3d_proj = (loss_bbox_y3d_proj * bbox_weights[active]).mean()\n",
    "                loss_bbox_z3d_proj = (loss_bbox_z3d_proj * bbox_weights[active]).mean()\n",
    "\n",
    "                bbox_3d_proj_loss = (loss_bbox_x3d_proj + loss_bbox_y3d_proj + loss_bbox_z3d_proj)\n",
    "\n",
    "                bbox_3d_proj_loss *= self.bbox_3d_proj_lambda\n",
    "\n",
    "                loss += bbox_3d_proj_loss\n",
    "                stats.append({'name': 'bbox_3d_proj', 'val': bbox_3d_proj_loss, 'format': '{:0.4f}', 'group': 'loss'})\n",
    "\n",
    "            coords_abs_z = coords_abs_z.view(-1)\n",
    "            stats.append({'name': 'z', 'val': coords_abs_z[active].mean(), 'format': '{:0.2f}', 'group': 'misc'})\n",
    "\n",
    "            coords_abs_ry = coords_abs_ry.view(-1)\n",
    "            stats.append({'name': 'ry', 'val': coords_abs_ry[active].mean(), 'format': '{:0.2f}', 'group': 'misc'})\n",
    "\n",
    "            ious_2d = ious_2d.view(-1)\n",
    "            stats.append({'name': 'iou', 'val': ious_2d[active].mean(), 'format': '{:0.2f}', 'group': 'acc'})\n",
    "\n",
    "            # use a 2d IoU based log loss\n",
    "            if self.iou_2d_lambda:\n",
    "                iou_2d_loss = -torch.log(ious_2d[active])\n",
    "                iou_2d_loss= torch.tensor(iou_2d_loss, device= self.device)\n",
    "                iou_2d_loss = (iou_2d_loss * bbox_weights[active])\n",
    "                iou_2d_loss = iou_2d_loss.mean()\n",
    "\n",
    "                iou_2d_loss *= self.iou_2d_lambda\n",
    "                loss += iou_2d_loss\n",
    "                \n",
    "                stats.append({'name': 'iou', 'val': iou_2d_loss, 'format': '{:0.4f}', 'group': 'loss'})\n",
    "\n",
    "            \n",
    "        return loss, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d31eb6",
   "metadata": {},
   "source": [
    "## Auxillary for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_detect_3d(im, net, rpn_conf, p2, synced=False):\n",
    "    \"\"\"\n",
    "    Object detection in 3D\n",
    "    \"\"\"\n",
    "\n",
    "    imH_orig = 1216\n",
    "    imW_orig = 1920\n",
    "    device= 'cuda:2'\n",
    "    #im = preprocess(im)\n",
    "\n",
    "    imH = im.shape[2]\n",
    "    imW = im.shape[3]\n",
    "\n",
    "    scale_factor = imH / imH_orig\n",
    "\n",
    "    cls, prob, bbox_2d, bbox_3d, feat_size, rois = net(im.float())\n",
    "    #print(bbox_3d[0])\n",
    "    # compute feature resolution\n",
    "    num_anchors = rpn_conf.anchors.shape[0]\n",
    "\n",
    "    bbox_x = (bbox_2d[:, :, 0]).to(device)\n",
    "    bbox_y = (bbox_2d[:, :, 1]).to(device)\n",
    "    bbox_w = (bbox_2d[:, :, 2]).to(device)\n",
    "    bbox_h = (bbox_2d[:, :, 3]).to(device)\n",
    "\n",
    "    bbox_x3d = (bbox_3d[:, :, 0]).to(device)\n",
    "    bbox_y3d = (bbox_3d[:, :, 1]).to(device)\n",
    "    bbox_z3d = (bbox_3d[:, :, 2]).to(device)\n",
    "    bbox_w3d = (bbox_3d[:, :, 3]).to(device)\n",
    "    bbox_h3d = (bbox_3d[:, :, 4]).to(device)\n",
    "    bbox_l3d = (bbox_3d[:, :, 5]).to(device)\n",
    "    bbox_ry3d = (bbox_3d[:, :, 6]).to(device)\n",
    "\n",
    "    # detransform 3d\n",
    "    bbox_x3d = bbox_x3d * rpn_conf.bbox_stds[:, 4][0] + rpn_conf.bbox_means[:, 4][0]\n",
    "    bbox_y3d = bbox_y3d * rpn_conf.bbox_stds[:, 5][0] + rpn_conf.bbox_means[:, 5][0]\n",
    "    bbox_z3d = bbox_z3d * rpn_conf.bbox_stds[:, 6][0] + rpn_conf.bbox_means[:, 6][0]\n",
    "    bbox_w3d = bbox_w3d * rpn_conf.bbox_stds[:, 7][0] + rpn_conf.bbox_means[:, 7][0]\n",
    "    bbox_h3d = bbox_h3d * rpn_conf.bbox_stds[:, 8][0] + rpn_conf.bbox_means[:, 8][0]\n",
    "    bbox_l3d = bbox_l3d * rpn_conf.bbox_stds[:, 9][0] + rpn_conf.bbox_means[:, 9][0]\n",
    "    bbox_ry3d = bbox_ry3d * rpn_conf.bbox_stds[:, 10][0] + rpn_conf.bbox_means[:, 10][0]\n",
    "\n",
    "    # find 3d source\n",
    "    tracker = rois[:, 4].cpu().detach().numpy().astype(np.int64)\n",
    "    src_3d = torch.from_numpy(rpn_conf.anchors[tracker, 4:]).to(device).type(torch.cuda.FloatTensor)\n",
    "\n",
    "    #tracker_sca = rois_sca[:, 4].cpu().detach().numpy().astype(np.int64)\n",
    "    #src_3d_sca = torch.from_numpy(rpn_conf.anchors[tracker_sca, 4:]).cuda().type(torch.cuda.FloatTensor)\n",
    "\n",
    "    # compute 3d transform\n",
    "    widths = rois[:, 2] - rois[:, 0] + 1.0\n",
    "    heights = rois[:, 3] - rois[:, 1] + 1.0\n",
    "    ctr_x = rois[:, 0] + 0.5 * widths\n",
    "    ctr_y = rois[:, 1] + 0.5 * heights\n",
    "\n",
    "    bbox_x3d = bbox_x3d[0, :] * widths + ctr_x\n",
    "    bbox_y3d = bbox_y3d[0, :] * heights + ctr_y\n",
    "    bbox_z3d = src_3d[:, 0] + bbox_z3d[0, :]\n",
    "    bbox_w3d = torch.exp(bbox_w3d[0, :]) * src_3d[:, 1]\n",
    "    bbox_h3d = torch.exp(bbox_h3d[0, :]) * src_3d[:, 2]\n",
    "    bbox_l3d = torch.exp(bbox_l3d[0, :]) * src_3d[:, 3]\n",
    "    bbox_ry3d = src_3d[:, 4] + bbox_ry3d[0, :]\n",
    "\n",
    "    # bundle\n",
    "    coords_3d = torch.stack((bbox_x3d, bbox_y3d, bbox_z3d[:bbox_x3d.shape[0]], bbox_w3d[:bbox_x3d.shape[0]], bbox_h3d[:bbox_x3d.shape[0]], bbox_l3d[:bbox_x3d.shape[0]], bbox_ry3d[:bbox_x3d.shape[0]]), dim=1)\n",
    "\n",
    "    # compile deltas pred\n",
    "    deltas_2d = torch.cat((bbox_x[0, :, np.newaxis], bbox_y[0, :, np.newaxis], bbox_w[0, :, np.newaxis], bbox_h[0, :, np.newaxis]), dim=1)\n",
    "    coords_2d = bbox_transform_inv(rois, deltas_2d, means=rpn_conf.bbox_means[0, :], stds=rpn_conf.bbox_stds[0, :])\n",
    "\n",
    "    # detach onto cpu\n",
    "    coords_2d = coords_2d.cpu().detach().numpy()\n",
    "    coords_3d = coords_3d.cpu().detach().numpy()\n",
    "    prob = prob[0, :, :].cpu().detach().numpy()\n",
    "\n",
    "    # scale coords\n",
    "    coords_2d[:, 0:4] /= scale_factor\n",
    "    coords_3d[:, 0:2] /= scale_factor\n",
    "\n",
    "    cls_pred = np.argmax(prob[:, 1:], axis=1) + 1\n",
    "    scores = np.amax(prob[:, 1:], axis=1)\n",
    "\n",
    "    aboxes = np.hstack((coords_2d, scores[:, np.newaxis]))\n",
    "\n",
    "    sorted_inds = (-aboxes[:, 4]).argsort()\n",
    "    original_inds = (sorted_inds).argsort()\n",
    "    aboxes = aboxes[sorted_inds, :]\n",
    "    coords_3d = coords_3d[sorted_inds, :]\n",
    "    cls_pred = cls_pred[sorted_inds]\n",
    "    tracker = tracker[sorted_inds]\n",
    "\n",
    "    if synced:\n",
    "\n",
    "        # nms\n",
    "        keep_inds = py_cpu_nms(aboxes[:, 0:5].astype(np.float32), rpn_conf.nms_thres)\n",
    "\n",
    "        # convert to bool\n",
    "        keep = np.zeros([aboxes.shape[0], 1], dtype=bool)\n",
    "        keep[keep_inds, :] = True\n",
    "\n",
    "        # stack the keep array,\n",
    "        # sync to the original order\n",
    "        aboxes = np.hstack((aboxes, keep))\n",
    "        aboxes[original_inds, :]\n",
    "\n",
    "    else:\n",
    "\n",
    "        # pre-nms\n",
    "        cls_pred = cls_pred[0:min(rpn_conf.nms_topN_pre, cls_pred.shape[0])]\n",
    "        tracker = tracker[0:min(rpn_conf.nms_topN_pre, tracker.shape[0])]\n",
    "        aboxes = aboxes[0:min(rpn_conf.nms_topN_pre, aboxes.shape[0]), :]\n",
    "        coords_3d = coords_3d[0:min(rpn_conf.nms_topN_pre, coords_3d.shape[0])]\n",
    "\n",
    "        # nms\n",
    "        keep_inds = py_cpu_nms(aboxes[:, 0:5].astype(np.float32), rpn_conf.nms_thres)\n",
    "\n",
    "        # stack cls prediction\n",
    "        aboxes = np.hstack((aboxes, cls_pred[:, np.newaxis], coords_3d, tracker[:, np.newaxis]))\n",
    "\n",
    "        # suppress boxes\n",
    "        aboxes = aboxes[keep_inds, :]\n",
    "\n",
    "    # clip boxes\n",
    "    if rpn_conf.clip_boxes:\n",
    "        aboxes[:, 0] = np.clip(aboxes[:, 0], 0, imW_orig - 1)\n",
    "        aboxes[:, 1] = np.clip(aboxes[:, 1], 0, imH_orig - 1)\n",
    "        aboxes[:, 2] = np.clip(aboxes[:, 2], 0, imW_orig - 1)\n",
    "        aboxes[:, 3] = np.clip(aboxes[:, 3], 0, imH_orig - 1)\n",
    "\n",
    "    return aboxes\n",
    "\n",
    "\n",
    "def hill_climb(p2, p2_inv, box_2d, x2d, y2d, z2d, w3d, h3d, l3d, ry3d, step_z_init=0, step_r_init=0, z_lim=0, r_lim=0, min_ol_dif=0.0):\n",
    "\n",
    "    step_z = step_z_init\n",
    "    step_r = step_r_init\n",
    "\n",
    "    ol_best, verts_best, _, invalid = test_projection(p2, p2_inv, box_2d, x2d, y2d, z2d, w3d, h3d, l3d, ry3d)\n",
    "\n",
    "    if invalid: return z2d, ry3d, verts_best\n",
    "\n",
    "    # attempt to fit z/rot more properly\n",
    "    while (step_z > z_lim or step_r > r_lim):\n",
    "        \n",
    "        if step_z > z_lim:\n",
    "\n",
    "            ol_neg, verts_neg, _, invalid_neg = test_projection(p2, p2_inv, box_2d, x2d, y2d, z2d - step_z, w3d, h3d, l3d, ry3d)\n",
    "            ol_pos, verts_pos, _, invalid_pos = test_projection(p2, p2_inv, box_2d, x2d, y2d, z2d + step_z, w3d, h3d, l3d, ry3d)\n",
    "\n",
    "            invalid = ((ol_pos - ol_best) <= min_ol_dif) and ((ol_neg - ol_best) <= min_ol_dif)\n",
    "\n",
    "            if invalid:\n",
    "                step_z = step_z * 0.5\n",
    "\n",
    "            elif (ol_pos - ol_best) > min_ol_dif and ol_pos > ol_neg and not invalid_pos:\n",
    "                z2d += step_z\n",
    "                ol_best = ol_pos\n",
    "                verts_best = verts_pos\n",
    "            elif (ol_neg - ol_best) > min_ol_dif and not invalid_neg:\n",
    "                z2d -= step_z\n",
    "                ol_best = ol_neg\n",
    "                verts_best = verts_neg\n",
    "            else:\n",
    "                step_z = step_z * 0.5\n",
    "\n",
    "        if step_r > r_lim:\n",
    "\n",
    "            ol_neg, verts_neg, _, invalid_neg = test_projection(p2, p2_inv, box_2d, x2d, y2d, z2d, w3d, h3d, l3d, ry3d - step_r)\n",
    "            ol_pos, verts_pos, _, invalid_pos = test_projection(p2, p2_inv, box_2d, x2d, y2d, z2d, w3d, h3d, l3d, ry3d + step_r)\n",
    "\n",
    "            invalid = ((ol_pos - ol_best) <= min_ol_dif) and ((ol_neg - ol_best) <= min_ol_dif)\n",
    "\n",
    "            if invalid:\n",
    "                step_r = step_r * 0.5\n",
    "\n",
    "            elif (ol_pos - ol_best) > min_ol_dif and ol_pos > ol_neg and not invalid_pos:\n",
    "                ry3d += step_r\n",
    "                ol_best = ol_pos\n",
    "                verts_best = verts_pos\n",
    "            elif (ol_neg - ol_best) > min_ol_dif and not invalid_neg:\n",
    "                ry3d -= step_r\n",
    "                ol_best = ol_neg\n",
    "                verts_best = verts_neg\n",
    "            else:\n",
    "                step_r = step_r * 0.5\n",
    "\n",
    "    while ry3d > math.pi: ry3d -= math.pi * 2\n",
    "    while ry3d < (-math.pi): ry3d += math.pi * 2\n",
    "\n",
    "    return z2d, ry3d, verts_best\n",
    "\n",
    "\n",
    "def test_projection(p2, p2_inv, box_2d, cx, cy, z, w3d, h3d, l3d, rotY):\n",
    "    \"\"\"\n",
    "    Tests the consistency of a 3D projection compared to a 2D box\n",
    "    \"\"\"\n",
    "\n",
    "    x = box_2d[0]\n",
    "    y = box_2d[1]\n",
    "    x2 = x + box_2d[2] - 1\n",
    "    y2 = y + box_2d[3] - 1\n",
    "    \n",
    "    p2_inv= torch.squeeze(p2_inv, dim=0)\n",
    "    #print(p2_inv.shape)\n",
    "    #coord3d = p2_inv.dot(np.array([cx * z, cy * z, z, 1]))\n",
    "    #coord3d = p2_inv.dot(torch.tensor(np.array([cx * z, cy * z, z, 1])))\n",
    "    coord3d= torch.matmul(p2_inv,(torch.tensor(np.array([cx * z, cy * z, z, 1]))))\n",
    "    print(coord3d.size())\n",
    "    cx3d = coord3d[0]\n",
    "    cy3d = coord3d[1]\n",
    "    cz3d = coord3d[2]\n",
    "\n",
    "    # put back on ground first\n",
    "    #cy3d += h3d/2\n",
    "\n",
    "    # re-compute the 2D box using 3D (finally, avoids clipped boxes)\n",
    "    verts3d, corners_3d = project_3d(p2, cx3d, cy3d, cz3d, w3d, h3d, l3d, rotY, return_3d=True)\n",
    "\n",
    "    invalid = np.any(corners_3d[2, :] <= 0)\n",
    "\n",
    "    x_new = min(verts3d[:, 0])\n",
    "    y_new = min(verts3d[:, 1])\n",
    "    x2_new = max(verts3d[:, 0])\n",
    "    y2_new = max(verts3d[:, 1])\n",
    "\n",
    "    b1 = np.array([x, y, x2, y2])[np.newaxis, :]\n",
    "    b2 = np.array([x_new, y_new, x2_new, y2_new])[np.newaxis, :]\n",
    "\n",
    "    #ol = iou(b1, b2)[0][0]\n",
    "    ol = -(np.abs(x - x_new) + np.abs(y - y_new) + np.abs(x2 - x2_new) + np.abs(y2 - y2_new))\n",
    "\n",
    "    return ol, verts3d, b2, invalid\n",
    "def convertAlpha2Rot(alpha, z3d, x3d):\n",
    "\n",
    "    ry3d = alpha + math.atan2(-z3d, x3d) + 0.5 * math.pi\n",
    "    #ry3d = alpha + math.atan2(x3d, z3d)# + 0.5 * math.pi\n",
    "\n",
    "    while ry3d > math.pi: ry3d -= math.pi * 2\n",
    "    while ry3d < (-math.pi): ry3d += math.pi * 2\n",
    "\n",
    "    return ry3d\n",
    "def project_3d(p2, x3d, y3d, z3d, w3d, h3d, l3d, ry3d, return_3d=False):\n",
    "    \"\"\"\n",
    "    Projects a 3D box into 2D vertices\n",
    "    Args:\n",
    "        p2 (nparray): projection matrix of size 4x3\n",
    "        x3d: x-coordinate of center of object\n",
    "        y3d: y-coordinate of center of object\n",
    "        z3d: z-cordinate of center of object\n",
    "        w3d: width of object\n",
    "        h3d: height of object\n",
    "        l3d: length of object\n",
    "        ry3d: rotation w.r.t y-axis\n",
    "    \"\"\"\n",
    "\n",
    "    # compute rotational matrix around yaw axis\n",
    "    R = np.array([[+math.cos(ry3d), -math.sin(ry3d),0],\n",
    "                  \n",
    "                  [+math.sin(ry3d), +math.cos(ry3d),0],[0, 0,1]])\n",
    "\n",
    "    # 3D bounding box corners\n",
    "    x_corners = np.array([0, l3d, l3d, l3d, l3d,   0,   0,   0])\n",
    "    y_corners = np.array([0, 0,   h3d, h3d,   0,   0, h3d, h3d])\n",
    "    z_corners = np.array([0, 0,     0, w3d, w3d, w3d, w3d,   0])\n",
    "\n",
    "    x_corners += -l3d / 2\n",
    "    y_corners += -h3d / 2\n",
    "    z_corners += -w3d / 2\n",
    "\n",
    "    # bounding box in object co-ordinate\n",
    "    corners_3d = np.array([x_corners, y_corners, z_corners])\n",
    "    \n",
    "    # rotate\n",
    "    corners_3d = R.dot(corners_3d)\n",
    "    \n",
    "    # translate\n",
    "    corners_3d = corners_3d + np.array([x3d, y3d, z3d]).reshape((3, 1))\n",
    "    \n",
    "    corners_3D_1 = np.vstack((corners_3d, np.ones((corners_3d.shape[-1]))))\n",
    "    \n",
    "    #corners_2D = p2.dot(corners_3D_1)\n",
    "    corners_2D= np.matmul(p2,corners_3D_1)\n",
    "    corners_2D= np.squeeze(corners_2D)\n",
    "    corners_2D = corners_2D // corners_2D[2,:]\n",
    "    #corners_2D= np.divide(corners_2D.T, (corners_2D[2,:]).T)\n",
    "    #print(\"corners_2D\",corners_2D)\n",
    "    bb3d_lines_verts_idx = [0, 1, 2, 3, 4, 5, 6, 7, 0, 5, 4, 1, 2, 7, 6, 3]\n",
    "\n",
    "    verts3d = (corners_2D[:, bb3d_lines_verts_idx][:2]).T\n",
    "\n",
    "    if return_3d:\n",
    "        return verts3d, corners_3d\n",
    "    else:\n",
    "        return verts3d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c692d0",
   "metadata": {},
   "source": [
    "## Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11789f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_2d_box(im, box, color=(0, 200, 200), thickness=3):\n",
    "\n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    x2 = box[2]\n",
    "    y2 = box[3]\n",
    "    #x2 = (x + w) - 1\n",
    "    #y2 = (y + h) - 1\n",
    "\n",
    "    cv2.rectangle(im, (int(x), int(y)), (int(x2), int(y2)), color, thickness)\n",
    "    #plt.imshow(im)\n",
    "    return im\n",
    "def draw_3d_box(im, verts, color=(0, 200, 200), thickness=3):\n",
    "\n",
    "    for lind in range(0, verts.shape[0] - 1):\n",
    "        v1 = (verts[lind])*0.25\n",
    "        v2 = (verts[lind + 1])*0.25 \n",
    "        cv2.line(im, (int(v1[0]), int(v1[1])), (int(v2[0]), int(v2[1])), color, thickness)\n",
    "    #plt.imshow(im)\n",
    "    return im\n",
    "def draw_circle(im, pos, radius=5, thickness=3, color=(0, 200, 200), fill=False):\n",
    "\n",
    "    if fill: thickness = -1\n",
    "\n",
    "    cv2.circle(im, (int(pos[0]), int(pos[1])), radius, color=color, thickness=thickness)\n",
    "    #plt.imshow(im)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d181f246",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_iteration(loader, iterator):\n",
    "    \"\"\"\n",
    "    Loads the next iteration of 'iterator' OR makes a new epoch using 'loader'.\n",
    "    Args:\n",
    "        loader (object): PyTorch DataLoader object\n",
    "        iterator (object): python in-built iter(loader) object\n",
    "    \"\"\"\n",
    "\n",
    "    # create if none\n",
    "    if iterator == None: iterator = iter(loader)\n",
    "\n",
    "    # next batch\n",
    "    try:\n",
    "        imobjs = next(iterator)\n",
    "\n",
    "    # new epoch / shuffle\n",
    "    except StopIteration:\n",
    "        iterator = iter(loader)\n",
    "        imobjs = next(iterator)\n",
    "\n",
    "    return iterator, imobjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35b8c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_det = RPN_3D_loss(conf)\n",
    "criterion_det= criterion_det.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f758370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 200\n",
    "iterator = None\n",
    "start_iter = 0\n",
    "device= 'cuda:2'\n",
    "#scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    i=0\n",
    "    print(\"Epoch:{}\".format(epoch))\n",
    "    total_loss=0\n",
    "    #total_loss_cls=0\n",
    "    #total_iou_2d_loss=0\n",
    "    #total_bbox_3d_loss=0\n",
    "    for iteration in range(start_iter, len(train_dataset)):\n",
    "        #print(\"{}/{}\".format(iteration,len(train_dataset)))\n",
    "        iterator, imobjs = next_iteration(train_data, iterator)\n",
    "        image, gts= imobjs.image, imobjs.gts\n",
    "        image= image.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        cls, prob, bbox_2d, bbox_3d, feat_size = rpn_net(image.float())\n",
    "        #print(cls.shape)\n",
    "        loss, stats = criterion_det(cls, prob, bbox_2d, bbox_3d, imobjs, feat_size)\n",
    "        print(\"{}/{}, loss: {}\".format(iteration,len(train_dataset), loss))\n",
    "        L= loss.detach().cpu()\n",
    "        if ((loss == math.inf) or (np.isnan(L)== True)):\n",
    "            i+=1\n",
    "            pass\n",
    "        if loss>0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #scaler.scale(loss).backward()\n",
    "            #scaler.step(optimizer)\n",
    "            #scaler.update()\n",
    "            total_loss+= loss           \n",
    "        training_loss= total_loss/(len(train_dataset) -i)\n",
    "    print('training_loss',training_loss )\n",
    "    print('i',i)\n",
    "    \n",
    "    writer.add_scalar(\"Training_loss\",training_loss, epoch )\n",
    "\n",
    "   \n",
    "    if (epoch % 5  == 0):\n",
    "        \n",
    "        model_path= r\"E:\\Rishav_Thesis\\Baseline\\Monocular_3D_Object_Det\\ordered\\Saved_model_final\\model_parameter_%d.pth\"%epoch\n",
    "        torch.save( rpn_net.state_dict(), model_path)\n",
    "     \n",
    "    if (epoch %3==0):\n",
    "        rpn_net.eval()\n",
    "        total_loss=0\n",
    "\n",
    "        for iteration in range(start_iter, len(val_dataset)):\n",
    "            \n",
    "            iterator, imobjs = next_iteration(val_data, iterator)\n",
    "            image, gts= imobjs.image, imobjs.gts\n",
    "            image= image.to(device)\n",
    "            cls, prob, bbox_2d, bbox_3d, feat_size,rois = rpn_net(image.float())\n",
    "            loss, stats = criterion_det(cls, prob, bbox_2d, bbox_3d, imobjs, feat_size)\n",
    "            print(\"{}/{}, loss: {}\".format(iteration,len(val_dataset), loss))\n",
    "            total_loss+= loss    \n",
    "            \n",
    "\n",
    "            \n",
    "        val_loss= total_loss/len(val_dataset)\n",
    "        print('val_loss', val_loss)\n",
    "        writer.add_scalar(\"Validation_loss\",val_loss, epoch )\n",
    "        rpn_net.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e698ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4d20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec388036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
